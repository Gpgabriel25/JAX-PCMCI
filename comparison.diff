diff --git a/README.md b/README.md
index e9a9e70..9308be3 100644
--- a/README.md
+++ b/README.md
@@ -8,6 +8,8 @@
 
 Author's note: Hello, what brings you to this program? If you are here, I would love to hear your thoughts on this library and how you are using it. Just send me an email anytime. If you have any issues with it, please open an issue, or just tell me. I will likely get it fixed somewhat quickly.
 
+Also, side note. A lot of the performance is based off of the parameters. So changes like batch size, tau, precision, or any other parameters can make a huge difference to speed.
+
 JAX-PCMCI is a library for causal discovery from time series data, implementing the PCMCI family of algorithms with GPU/TPU acceleration through JAX. It provides significant speedups over CPU-based implementations while maintaining scientific rigor.
 
 ## Key Features
@@ -288,13 +290,15 @@ PCMCI+ extends PCMCI to handle contemporaneous (Ï„=0) causal links:
 2. **Orientation**: Uses time order and v-structure rules to orient edges
 3. **MCI Testing**: Final tests with full conditioning sets
 
-## ðŸ§ª Comparison with Tigramite
+##  Comparison with Tigramite
+
+
 
 | Feature | JAX-PCMCI | Tigramite |
 |---------|-----------|-----------|
-| GPU/TPU Support | âœ… Native | âŒ CPU only |
-| Parallelization | âœ… vmap/pmap | âš ï¸ Limited |
-| JIT Compilation | âœ… Full | âŒ No |
+| GPU/TPU Support |  Native |  CPU only |
+| Parallelization |  vmap/pmap |  Limited |
+| JIT Compilation |  Full |  No |
 | Independence Tests | ParCorr, CMI, GPDC | Many |
 | Speed (GPU) | 10-100x faster | Baseline |
 
@@ -311,6 +315,6 @@ PCMCI+ extends PCMCI to handle contemporaneous (Ï„=0) causal links:
 MIT License - see [LICENSE](LICENSE) for details.
 
 
-## ðŸ“§ Contact
+## Contact
 
 For questions or issues, please open a GitHub issue or contact me at gpgabriel25@gmail.com
diff --git a/examples/__pycache__/example_linear_var.cpython-313.pyc b/examples/__pycache__/example_linear_var.cpython-313.pyc
new file mode 100644
index 0000000..c623579
Binary files /dev/null and b/examples/__pycache__/example_linear_var.cpython-313.pyc differ
diff --git a/examples/__pycache__/example_nonlinear.cpython-313.pyc b/examples/__pycache__/example_nonlinear.cpython-313.pyc
new file mode 100644
index 0000000..c7ff88f
Binary files /dev/null and b/examples/__pycache__/example_nonlinear.cpython-313.pyc differ
diff --git a/examples/__pycache__/example_pcmci_plus.cpython-313.pyc b/examples/__pycache__/example_pcmci_plus.cpython-313.pyc
new file mode 100644
index 0000000..8c12e12
Binary files /dev/null and b/examples/__pycache__/example_pcmci_plus.cpython-313.pyc differ
diff --git a/examples/benchmark_pcmci_speed.py b/examples/benchmark_pcmci_speed.py
new file mode 100644
index 0000000..d02aa9b
--- /dev/null
+++ b/examples/benchmark_pcmci_speed.py
@@ -0,0 +1,173 @@
+"""
+Benchmark: PCMCI and PCMCI+ end-to-end speed.
+
+Environment variables:
+- PCMCI_SPEED_T: time points (default 1000)
+- PCMCI_SPEED_N: variables (default 10)
+- PCMCI_SPEED_TAU_MAX: maximum lag (default 2)
+- PCMCI_SPEED_PC_ALPHA: PC alpha (default 0.05)
+- PCMCI_SPEED_ALPHA_LEVEL: MCI alpha (default 0.05)
+- PCMCI_SPEED_DEVICE: cpu|gpu|tpu|auto (default auto)
+- PCMCI_SPEED_WARMUP: 1 to run a warmup pass (default 1)
+"""
+
+from __future__ import annotations
+
+import os
+import time
+import resource
+
+import jax
+import jax.numpy as jnp
+
+from jax_pcmci import (
+    DataHandler,
+    ParCorr,
+    PCMCI,
+    PCMCIPlus,
+    PCMCIConfig,
+    get_device_info,
+    set_device,
+)
+
+
+def _env_int(name: str, default: int) -> int:
+    return int(os.environ.get(name, default))
+
+
+def _env_float(name: str, default: float) -> float:
+    return float(os.environ.get(name, default))
+
+
+def _env_bool(name: str, default: bool) -> bool:
+    val = os.environ.get(name)
+    if val is None:
+        return default
+    return val.lower() in {"1", "true", "yes"}
+
+
+def _get_cpu_mem_mb() -> float:
+    try:
+        import psutil  # type: ignore
+
+        return psutil.Process().memory_info().rss / 1e6
+    except Exception:
+        # ru_maxrss is KB on Linux
+        return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024.0
+
+
+def _get_gpu_mem_stats() -> dict:
+    stats = {}
+    try:
+        dev = jax.devices()[0]
+        if hasattr(dev, "memory_stats"):
+            mem = dev.memory_stats() or {}
+            for key in ("bytes_in_use", "peak_bytes_in_use", "bytes_reserved"):
+                if key in mem:
+                    stats[key] = mem[key]
+    except Exception:
+        pass
+    return stats
+
+
+def _format_mem_stats(cpu_mb: float, gpu_stats: dict) -> str:
+    parts = [f"CPU RSS: {cpu_mb:.1f} MB"]
+    if gpu_stats:
+        if "bytes_in_use" in gpu_stats:
+            parts.append(f"GPU in use: {gpu_stats['bytes_in_use'] / 1e6:.1f} MB")
+        if "peak_bytes_in_use" in gpu_stats:
+            parts.append(f"GPU peak: {gpu_stats['peak_bytes_in_use'] / 1e6:.1f} MB")
+        if "bytes_reserved" in gpu_stats:
+            parts.append(f"GPU reserved: {gpu_stats['bytes_reserved'] / 1e6:.1f} MB")
+    return " | ".join(parts)
+
+
+def time_run(label: str, fn) -> float:
+    cpu_before = _get_cpu_mem_mb()
+    gpu_before = _get_gpu_mem_stats()
+    start = time.perf_counter()
+    result = fn()
+    if hasattr(result, "val_matrix"):
+        jax.block_until_ready(result.val_matrix)
+    elapsed = time.perf_counter() - start
+    cpu_after = _get_cpu_mem_mb()
+    gpu_after = _get_gpu_mem_stats()
+    print(f"{label}: {elapsed:.3f}s")
+    print(f"{label} memory: {_format_mem_stats(cpu_after, gpu_after)}")
+    return elapsed
+
+
+def main() -> None:
+    t = _env_int("PCMCI_SPEED_T", 500)
+    n = _env_int("PCMCI_SPEED_N", 10)
+    tau_max = _env_int("PCMCI_SPEED_TAU_MAX", 2)
+    pc_alpha = _env_float("PCMCI_SPEED_PC_ALPHA", 0.05)
+    alpha_level = _env_float("PCMCI_SPEED_ALPHA_LEVEL", 0.05)
+    do_warmup = _env_bool("PCMCI_SPEED_WARMUP", True)
+    max_conds_dim = _env_int("PCMCI_SPEED_MAX_CONDS_DIM", 3)
+
+    device = os.environ.get("PCMCI_SPEED_DEVICE", "auto")
+    set_device(device)
+
+    config = PCMCIConfig()
+    config.apply()
+
+    info = get_device_info()
+    print("=" * 60)
+    print("PCMCI Speed Benchmark")
+    print("=" * 60)
+    print(f"Device: {info['default_backend']}")
+    print(f"T={t}, N={n}, tau_max={tau_max}")
+
+    key = jax.random.PRNGKey(0)
+    data = jax.random.normal(key, (t, n))
+    handler = DataHandler(data)
+    test = ParCorr()
+
+    if do_warmup:
+        # Full warmup: run PCMCI once with actual parameters to JIT compile
+        # all necessary configurations. This simulates real-world usage where
+        # the algorithm is run multiple times on similar data.
+        print("Warming up JIT compilation...")
+        warm_pcmci = PCMCI(handler, cond_ind_test=test, verbosity=0)
+        _ = warm_pcmci.run(
+            tau_max=tau_max,
+            pc_alpha=pc_alpha,
+            max_conds_dim=max_conds_dim,
+        )
+        warm_pcmci_plus = PCMCIPlus(handler, cond_ind_test=test, verbosity=0)
+        _ = warm_pcmci_plus.run(
+            tau_max=tau_max,
+            pc_alpha=pc_alpha,
+            max_conds_dim=max_conds_dim,
+        )
+        handler.clear_cache()
+        handler.precompute_lagged_data(tau_max)
+        print("Warmup complete. Running timed benchmarks...")
+
+    pcmci = PCMCI(handler, cond_ind_test=test, verbosity=0)
+    pcmci_plus = PCMCIPlus(handler, cond_ind_test=test, verbosity=0)
+
+    time_run(
+        "PCMCI",
+        lambda: pcmci.run(
+            tau_max=tau_max,
+            pc_alpha=pc_alpha,
+            alpha_level=alpha_level,
+            max_conds_dim=max_conds_dim,
+        ),
+    )
+
+    time_run(
+        "PCMCI+",
+        lambda: pcmci_plus.run(
+            tau_max=tau_max,
+            pc_alpha=pc_alpha,
+            alpha_level=alpha_level,
+            max_conds_dim=max_conds_dim,
+        ),
+    )
+
+
+if __name__ == "__main__":
+    main()
diff --git a/examples/example_benchmarks.py b/examples/example_benchmarks.py
index 456de09..0aa8cfb 100644
--- a/examples/example_benchmarks.py
+++ b/examples/example_benchmarks.py
@@ -6,6 +6,8 @@ This example benchmarks JAX-PCMCI performance across different
 configurations to help you choose optimal settings for your hardware.
 """
 
+from __future__ import annotations
+
 import os
 import signal
 import time
diff --git a/examples/example_eeg_connectivity.py b/examples/example_eeg_connectivity.py
index f067404..e4b614a 100644
--- a/examples/example_eeg_connectivity.py
+++ b/examples/example_eeg_connectivity.py
@@ -12,6 +12,8 @@ This is a stress test for:
 - GPU utilization with large matrices
 """
 
+from __future__ import annotations
+
 import jax
 import jax.numpy as jnp
 import numpy as np
diff --git a/examples/example_linear_var.py b/examples/example_linear_var.py
index 498858d..2b40ccc 100644
--- a/examples/example_linear_var.py
+++ b/examples/example_linear_var.py
@@ -9,6 +9,8 @@ The ground truth causal structure is known, allowing us to verify
 the accuracy of the discovered links.
 """
 
+from __future__ import annotations
+
 import jax
 import jax.numpy as jnp
 import numpy as np
diff --git a/examples/example_nonlinear.py b/examples/example_nonlinear.py
index 4f6d0ca..a36cf6f 100644
--- a/examples/example_nonlinear.py
+++ b/examples/example_nonlinear.py
@@ -9,6 +9,8 @@ in data with nonlinear dependencies.
 Linear tests like ParCorr would fail to detect these relationships.
 """
 
+from __future__ import annotations
+
 import jax
 import jax.numpy as jnp
 import numpy as np
diff --git a/examples/example_pcmci_plus.py b/examples/example_pcmci_plus.py
index ff9ff1f..f133f61 100644
--- a/examples/example_pcmci_plus.py
+++ b/examples/example_pcmci_plus.py
@@ -10,6 +10,8 @@ This is essential for systems where causation happens within
 the sampling interval.
 """
 
+from __future__ import annotations
+
 import jax
 import jax.numpy as jnp
 import numpy as np
diff --git a/jax_pcmci.egg-info/PKG-INFO b/jax_pcmci.egg-info/PKG-INFO
index 2e230aa..3c0b6c3 100644
--- a/jax_pcmci.egg-info/PKG-INFO
+++ b/jax_pcmci.egg-info/PKG-INFO
@@ -1,6 +1,6 @@
 Metadata-Version: 2.4
 Name: jax-pcmci
-Version: 1.0.0
+Version: 1.1.0
 Summary: High-performance causal discovery using PCMCI algorithms with JAX acceleration
 Author: JAX-PCMCI Contributors
 License: MIT
diff --git a/jax_pcmci/__init__.py b/jax_pcmci/__init__.py
index 7c6236e..c512518 100644
--- a/jax_pcmci/__init__.py
+++ b/jax_pcmci/__init__.py
@@ -39,7 +39,7 @@ For more examples and detailed documentation, visit:
 https://jax-pcmci.readthedocs.io
 """
 
-__version__ = "1.0.0"
+__version__ = "1.1.0"
 __author__ = "JAX-PCMCI Contributors"
 
 # Core imports
@@ -48,6 +48,7 @@ from jax_pcmci.independence_tests import (
     CondIndTest,
     ParCorr,
     CMIKnn,
+    CMISymbolic,
     GPDCond,
 )
 from jax_pcmci.algorithms import PCMCI, PCMCIPlus
@@ -70,6 +71,7 @@ __all__ = [
     "CondIndTest",
     "ParCorr",
     "CMIKnn",
+    "CMISymbolic",
     "GPDCond",
     # Algorithms
     "PCMCI",
diff --git a/jax_pcmci/__pycache__/__init__.cpython-313.pyc b/jax_pcmci/__pycache__/__init__.cpython-313.pyc
index 077f2cc..ff89583 100644
Binary files a/jax_pcmci/__pycache__/__init__.cpython-313.pyc and b/jax_pcmci/__pycache__/__init__.cpython-313.pyc differ
diff --git a/jax_pcmci/__pycache__/config.cpython-313.pyc b/jax_pcmci/__pycache__/config.cpython-313.pyc
index 405b854..3c07f58 100644
Binary files a/jax_pcmci/__pycache__/config.cpython-313.pyc and b/jax_pcmci/__pycache__/config.cpython-313.pyc differ
diff --git a/jax_pcmci/__pycache__/data.cpython-313.pyc b/jax_pcmci/__pycache__/data.cpython-313.pyc
index 1056aab..cca2c66 100644
Binary files a/jax_pcmci/__pycache__/data.cpython-313.pyc and b/jax_pcmci/__pycache__/data.cpython-313.pyc differ
diff --git a/jax_pcmci/__pycache__/parallel.cpython-313.pyc b/jax_pcmci/__pycache__/parallel.cpython-313.pyc
index d876c35..1e554eb 100644
Binary files a/jax_pcmci/__pycache__/parallel.cpython-313.pyc and b/jax_pcmci/__pycache__/parallel.cpython-313.pyc differ
diff --git a/jax_pcmci/__pycache__/results.cpython-313.pyc b/jax_pcmci/__pycache__/results.cpython-313.pyc
index f6220eb..9d8c136 100644
Binary files a/jax_pcmci/__pycache__/results.cpython-313.pyc and b/jax_pcmci/__pycache__/results.cpython-313.pyc differ
diff --git a/jax_pcmci/algorithms/__pycache__/pcmci.cpython-313.pyc b/jax_pcmci/algorithms/__pycache__/pcmci.cpython-313.pyc
index 42b52ef..d0701ee 100644
Binary files a/jax_pcmci/algorithms/__pycache__/pcmci.cpython-313.pyc and b/jax_pcmci/algorithms/__pycache__/pcmci.cpython-313.pyc differ
diff --git a/jax_pcmci/algorithms/__pycache__/pcmci_plus.cpython-313.pyc b/jax_pcmci/algorithms/__pycache__/pcmci_plus.cpython-313.pyc
index 97bc345..b1a5b85 100644
Binary files a/jax_pcmci/algorithms/__pycache__/pcmci_plus.cpython-313.pyc and b/jax_pcmci/algorithms/__pycache__/pcmci_plus.cpython-313.pyc differ
diff --git a/jax_pcmci/algorithms/pcmci.py b/jax_pcmci/algorithms/pcmci.py
index 6b3ff48..1fb59b9 100644
--- a/jax_pcmci/algorithms/pcmci.py
+++ b/jax_pcmci/algorithms/pcmci.py
@@ -43,9 +43,12 @@ from typing import Any, Dict, List, Optional, Sequence, Set, Tuple, Union
 
 import jax
 import jax.numpy as jnp
+import numpy as np
 from jax import lax
 from functools import partial
 from tqdm import tqdm
+from itertools import combinations
+import math
 
 from jax_pcmci.data import DataHandler
 from jax_pcmci.independence_tests.base import CondIndTest, TestResult
@@ -162,6 +165,108 @@ class PCMCI:
         self._parents: Dict[int, Set[Tuple[int, int]]] = {}
         self._pval_matrix: Optional[jax.Array] = None
         self._val_matrix: Optional[jax.Array] = None
+        self._batch_size_cache: Dict[Tuple[int, int], Optional[int]] = {}
+
+    def _sample_condition_subsets(
+        self,
+        items: List[Tuple[int, int]],
+        k: int,
+        max_subsets: int,
+        seed: int,
+    ) -> List[Tuple[Tuple[int, int], ...]]:
+        """
+        Reservoir-sample at most max_subsets k-sized combinations from items.
+
+        Falls back to full enumeration only when the total count is small
+        enough, avoiding large intermediate lists.
+        """
+        if k == 0:
+            return [tuple()]
+        if len(items) < k:
+            return []
+
+        total = math.comb(len(items), k)
+        if total <= max_subsets:
+            return list(combinations(items, k))
+
+        # Use NumPy for faster random sampling
+        rng = np.random.RandomState(seed)
+        reservoir: List[Tuple[Tuple[int, int], ...]] = []
+        
+        # Pre-sample indices for faster reservoir sampling
+        # Sample up front to reduce overhead
+        comb_iter = combinations(items, k)
+        for idx in range(max_subsets):
+            reservoir.append(next(comb_iter))
+        
+        # Continue with reservoir sampling for remaining
+        for idx in range(max_subsets, min(total, max_subsets * 10)):
+            try:
+                combo = next(comb_iter)
+                j = rng.randint(0, idx + 1)
+                if j < max_subsets:
+                    reservoir[j] = combo
+            except StopIteration:
+                break
+
+        return reservoir
+
+    def _get_effective_batch_size(self, n_samples: Optional[int] = None, n_conditions: int = 0) -> Optional[int]:
+        """
+        Resolve effective batch size for memory-aware batching.
+        
+        Dynamically computes batch size based on available GPU memory
+        if not explicitly configured.
+        
+        Parameters
+        ----------
+        n_samples : int, optional
+            Number of samples per test (for memory estimation).
+        n_conditions : int, default=0
+            Number of conditioning variables (for memory estimation).
+            
+        Returns
+        -------
+        int or None
+            Batch size, or None for unlimited batching.
+        """
+        config = get_config()
+        if config.batch_size is not None:
+            return config.batch_size
+        if config.memory_efficient:
+            return 256
+            
+        # Auto-compute batch size based on GPU memory
+        if n_samples is not None:
+            cache_key = (n_samples, n_conditions)
+            if cache_key in self._batch_size_cache:
+                return self._batch_size_cache[cache_key]
+            try:
+                device = jax.devices()[0]
+                if hasattr(device, 'memory_stats'):
+                    mem_stats = device.memory_stats() or {}
+                    # Use bytes_limit if available, otherwise estimate 8GB
+                    total_mem = mem_stats.get('bytes_limit', 8 * 1024**3)
+                    in_use = mem_stats.get('bytes_in_use', 0)
+                    available = (total_mem - in_use) * 0.7  # Use 70% of available
+                    
+                    # Estimate bytes per test: (X + Y + Z) * dtype_size * 3 (intermediates)
+                    dtype_size = jnp.dtype(config.dtype).itemsize
+                    bytes_per_test = n_samples * (2 + n_conditions) * dtype_size * 3
+                    
+                    if bytes_per_test > 0:
+                        computed_batch = max(64, int(available / bytes_per_test))
+                        batch = min(computed_batch, 8192)  # Cap at reasonable max
+                        self._batch_size_cache[cache_key] = batch
+                        return batch
+            except Exception:
+                pass  # Fall through to default
+
+            # Default safe batch size if memory stats unavailable or failed
+            self._batch_size_cache[cache_key] = 4096
+            return 4096
+        
+        return 4096
 
     def run(
         self,
@@ -254,6 +359,9 @@ class PCMCI:
         if tau_min < 0:
             raise ValueError(f"tau_min must be non-negative, got {tau_min}")
 
+        # Precompute lagged data to avoid repeated construction
+        self.datahandler.precompute_lagged_data(tau_max)
+
         # Initialize result matrices
         self._val_matrix = jnp.zeros((self.N, self.N, tau_max + 1))
         self._pval_matrix = jnp.ones((self.N, self.N, tau_max + 1))
@@ -272,6 +380,9 @@ class PCMCI:
             max_subsets=max_subsets,
         )
 
+        # Clear cache between phases to free memory
+        self.datahandler.clear_cache()
+
         # Phase 2: MCI tests
         if self.verbosity >= 1:
             print(f"\n{'='*60}")
@@ -393,90 +504,159 @@ class PCMCI:
             parents_snapshot = {j: parents[j].copy() for j in self.selected_variables}
             any_removed = False
 
-            # For cond_dim=0, use batch testing for efficiency
-            if cond_dim == 0 and hasattr(self.test, 'run_batch'):
+            # Check if we can use batch optimization
+            if hasattr(self.test, 'run_batch'):
+                # Global batching: collect tests from ALL variables first
+                # specs_by_lag stores: (j, parent, tau, subset)
+                specs_by_lag: Dict[int, List[Tuple[int, Tuple[int, int], int, Tuple[Tuple[int, int], ...]]]] = {}
+
                 for j in self.selected_variables:
                     current_parents = list(parents_snapshot[j])
                     if not current_parents:
                         continue
+
+                    if cond_dim == 0:
+                        # Unconditional optimization
+                        for parent in current_parents:
+                            i, neg_tau = parent
+                            tau = -neg_tau
+                            
+                            if tau not in specs_by_lag:
+                                specs_by_lag[tau] = []
+                            # Empty subset for cond_dim=0
+                            specs_by_lag[tau].append((j, parent, tau, tuple()))
                     
-                    # Group parents by lag (same lag = same data length)
-                    parents_by_lag: Dict[int, List[Tuple[int, int]]] = {}
-                    for parent in current_parents:
-                        i, neg_tau = parent
-                        tau = -neg_tau
-                        if tau not in parents_by_lag:
-                            parents_by_lag[tau] = []
-                        parents_by_lag[tau].append(parent)
-                    
-                    parents_to_remove = []
-                    
-                    # Process each lag group as a batch
-                    for tau, parents_with_tau in parents_by_lag.items():
-                        batch_data = []
-                        parent_list = []
-                        for parent in parents_with_tau:
-                            i, _ = parent
-                            X, Y, Z = self.datahandler.get_variable_pair_data(i, j, tau, None)
-                            batch_data.append((X, Y))
-                            parent_list.append(parent)
-                        
-                        # Run batch test for this lag group
-                        X_batch = jnp.stack([d[0] for d in batch_data])
-                        Y_batch = jnp.stack([d[1] for d in batch_data])
-                        stats, pvals = self.test.run_batch(X_batch, Y_batch, None, alpha=pc_alpha)
-                        
-                        # Mark non-significant (independent) parents for removal
-                        for idx, (val, pval) in enumerate(zip(stats, pvals)):
-                            if float(pval) > pc_alpha:  # Not significant = independent
-                                parents_to_remove.append(parent_list[idx])
-                                any_removed = True
-                    
-                    for parent in parents_to_remove:
-                        parents[j].discard(parent)
+                    else:
+                        # Conditional optimization
+                        if len(current_parents) <= cond_dim:
+                            continue
+                            
+                        for parent in current_parents:
+                            i, neg_tau = parent
+                            tau = -neg_tau
+                            
+                            other_parents = [p for p in current_parents if p != parent]
+                            if len(other_parents) < cond_dim:
+                                continue
+
+                            subsets_to_test = self._sample_condition_subsets(
+                                other_parents,
+                                cond_dim,
+                                max_subsets,
+                                seed=i * 1000 + j * 100 + tau + cond_dim,
+                            )
+
+                            for subset in subsets_to_test:
+                                max_lag_in_subset = max(-p[1] for p in subset) if subset else 0
+                                effective_max_lag = max(tau, max_lag_in_subset)
+                                if effective_max_lag not in specs_by_lag:
+                                    specs_by_lag[effective_max_lag] = []
+                                specs_by_lag[effective_max_lag].append((j, parent, tau, subset))
+
+                # Now run all collected tests in optimal batches
+                parents_to_remove_global = set() # Store (j, parent) tuples
+                
+                sorted_lags = sorted(specs_by_lag.keys())
+                for max_lag in sorted_lags:
+                    specs = specs_by_lag[max_lag]
                     
-                    if self.verbosity >= 2 and parents_to_remove:
-                        print(f"  Removed {len(parents_to_remove)} parents from X{j} (batch)")
-            else:
-                # Standard sequential testing for cond_dim > 0
-                for j in self.selected_variables:
-                    current_parents = list(parents_snapshot[j])
+                    effective_T = self.T - max_lag
+                    batch_size = self._get_effective_batch_size(
+                        n_samples=effective_T, n_conditions=cond_dim
+                    )
 
-                    if len(current_parents) <= cond_dim:
-                        continue
+                    if batch_size is None or batch_size > len(specs):
+                        chunk_ranges = [(0, len(specs))]
+                    else:
+                        chunk_ranges = [
+                            (start, min(start + batch_size, len(specs)))
+                            for start in range(0, len(specs), batch_size)
+                        ]
 
-                    # Test each parent
-                    parents_to_remove = []
-
-                    for parent in current_parents:
-                        i, neg_tau = parent
-                        tau = -neg_tau
-
-                        # Get possible conditioning sets (subsets of other parents)
-                        other_parents = [p for p in current_parents if p != parent]
-
-                        # Test with subsets of size cond_dim
-                        if len(other_parents) >= cond_dim:
-                            is_independent = self._test_with_conditioning_subsets(
-                                i=i,
-                                j=j,
-                                tau=tau,
-                                other_parents=other_parents,
-                                cond_dim=cond_dim,
-                                pc_alpha=pc_alpha,
-                                max_subsets=max_subsets,
+                    for start, end in chunk_ranges:
+                        batch_specs = specs[start:end]
+                        
+                        # Optimized unpacking
+                        j_list, parent_tuples, tau_list, subset_tuples = zip(*batch_specs)
+                        i_list = [p[0] for p in parent_tuples]
+                        
+                        i_arr = jnp.array(i_list, dtype=jnp.int32)
+                        j_arr = jnp.array(j_list, dtype=jnp.int32)
+                        tau_arr = jnp.array(tau_list, dtype=jnp.int32)
+                        
+                        if cond_dim > 0:
+                            # Shape: (batch, cond_dim, 2)
+                            subset_arr = np.array(subset_tuples, dtype=np.int32)
+                            cond_vars = jnp.array(subset_arr[:, :, 0], dtype=jnp.int32)
+                            cond_lags = jnp.array(-subset_arr[:, :, 1], dtype=jnp.int32)
+                            
+                            X_b, Y_b, Z_b = self.datahandler.get_variable_pair_batch(
+                                i_arr, j_arr, tau_arr, cond_vars, cond_lags, max_lag=max_lag
+                            )
+                        else:
+                            # Cond_dim = 0
+                            X_b, Y_b, Z_b = self.datahandler.get_variable_pair_batch(
+                                i_arr, j_arr, tau_arr, max_lag=max_lag
                             )
 
-                            if is_independent:
-                                parents_to_remove.append(parent)
-                                any_removed = True
-
-                    # Remove independent parents
-                    for parent in parents_to_remove:
-                        parents[j].discard(parent)
+                        stats, pvals = self.test.run_batch(X_b, Y_b, Z_b, alpha=pc_alpha)
+
+                        # Check for independence
+                        independent_mask = np.asarray(pvals > pc_alpha)
+                        for idx in np.where(independent_mask)[0]:
+                            j_idx = batch_specs[idx][0]
+                            p_idx = batch_specs[idx][1]
+                            parents_to_remove_global.add((j_idx, p_idx))
+                            any_removed = True
+                
+                # Apply removals
+                for j_idx, p_idx in parents_to_remove_global:
+                    parents[j_idx].discard(p_idx)
+                
+                if self.verbosity >= 2 and parents_to_remove_global:
+                    print(f"  Removed {len(parents_to_remove_global)} parents across all variables (cond_dim={cond_dim})")
 
-                    if self.verbosity >= 2 and parents_to_remove:
-                        print(f"  Removed {len(parents_to_remove)} parents from X{j}")
+            else:
+                # Sequential fallback (CPU/No-batch-support)
+                    # Sequential fallback
+                    for j in self.selected_variables:
+                        current_parents = list(parents_snapshot[j])
+
+                        if len(current_parents) <= cond_dim:
+                            continue
+
+                        # Test each parent
+                        parents_to_remove = []
+
+                        for parent in current_parents:
+                            i, neg_tau = parent
+                            tau = -neg_tau
+
+                            # Get possible conditioning sets (subsets of other parents)
+                            other_parents = [p for p in current_parents if p != parent]
+
+                            # Test with subsets of size cond_dim
+                            if len(other_parents) >= cond_dim:
+                                is_independent = self._test_with_conditioning_subsets(
+                                    i=i,
+                                    j=j,
+                                    tau=tau,
+                                    other_parents=other_parents,
+                                    cond_dim=cond_dim,
+                                    pc_alpha=pc_alpha,
+                                    max_subsets=max_subsets,
+                                )
+
+                                if is_independent:
+                                    parents_to_remove.append(parent)
+                                    any_removed = True
+
+                        # Remove independent parents
+                        for parent in parents_to_remove:
+                            parents[j].discard(parent)
+
+                        if self.verbosity >= 2 and parents_to_remove:
+                            print(f"  Removed {len(parents_to_remove)} parents from X{j}")
 
             if not any_removed:
                 # No removals - algorithm converged
@@ -512,24 +692,18 @@ class PCMCI:
             Maximum number of conditioning subsets to test. For large parent
             sets, randomly samples subsets instead of testing all C(n,k).
         """
-        from itertools import combinations
-        import random
-
         if cond_dim == 0:
             # Unconditional test
             X, Y, Z = self.datahandler.get_variable_pair_data(i, j, tau, None)
             result = self.test.run(X, Y, None, alpha=pc_alpha)
             return not result.significant
 
-        # Generate subsets - limit to max_subsets for large conditioning sets
-        all_subsets = list(combinations(other_parents, cond_dim))
-        
-        if len(all_subsets) > max_subsets:
-            # Randomly sample subsets for efficiency
-            random.seed(i * 1000 + j * 100 + tau)  # Reproducible
-            subsets_to_test = random.sample(all_subsets, max_subsets)
-        else:
-            subsets_to_test = all_subsets
+        subsets_to_test = self._sample_condition_subsets(
+            other_parents,
+            cond_dim,
+            max_subsets,
+            seed=i * 1000 + j * 100 + tau,
+        )
 
         # Group subsets by the max lag in their conditioning set for batching
         # (same max lag = same data length = can batch)
@@ -555,31 +729,47 @@ class PCMCI:
                     if not result.significant:
                         return True
                 else:
-                    # Batch test all subsets in this group
-                    X_batch = []
-                    Y_batch = []
-                    Z_batch = []
-                    
-                    for subset in subsets_group:
-                        condition_indices = [(var, -neg_lag) for var, neg_lag in subset]
-                        X, Y, Z = self.datahandler.get_variable_pair_data(
-                            i, j, tau, condition_indices
+                    # Batch test subsets in memory-aware chunks
+                    effective_T = self.T - max_lag
+                    batch_size = self._get_effective_batch_size(n_samples=effective_T, n_conditions=cond_dim)
+                    if batch_size is None:
+                        chunk_ranges = [(0, len(subsets_group))]
+                    else:
+                        chunk_ranges = [
+                            (start, min(start + batch_size, len(subsets_group)))
+                            for start in range(0, len(subsets_group), batch_size)
+                        ]
+
+                    for start, end in chunk_ranges:
+                        # Optimized subset unpacking
+                        current_subsets = subsets_group[start:end]
+                        
+                        # Convert to numpy array: (batch, cond_dim, 2)
+                        # entries are (var, neg_lag)
+                        subset_arr = np.array(current_subsets, dtype=np.int32)
+                        cond_vars = jnp.array(subset_arr[:, :, 0], dtype=jnp.int32)
+                        cond_lags = jnp.array(-subset_arr[:, :, 1], dtype=jnp.int32)
+
+                        batch_len = end - start
+                        i_arr = jnp.full((batch_len,), i, dtype=jnp.int32)
+                        j_arr = jnp.full((batch_len,), j, dtype=jnp.int32)
+                        tau_arr = jnp.full((batch_len,), tau, dtype=jnp.int32)
+
+                        X_arr, Y_arr, Z_arr = self.datahandler.get_variable_pair_batch(
+                            i_arr,
+                            j_arr,
+                            tau_arr,
+                            cond_vars=cond_vars,
+                            cond_lags=cond_lags,
+                            max_lag=max_lag,
                         )
-                        X_batch.append(X)
-                        Y_batch.append(Y)
-                        Z_batch.append(Z)
-                    
-                    # Stack into arrays
-                    X_arr = jnp.stack(X_batch, axis=0)
-                    Y_arr = jnp.stack(Y_batch, axis=0)
-                    Z_arr = jnp.stack(Z_batch, axis=0)
-                    
-                    # Run batch test
-                    stats, pvals = self.test.run_batch(X_arr, Y_arr, Z_arr, alpha=pc_alpha)
-                    
-                    # Check if any are independent (p-value > alpha)
-                    if bool(jnp.any(pvals > pc_alpha)):
-                        return True
+                        
+                        # Run batch test
+                        stats, pvals = self.test.run_batch(X_arr, Y_arr, Z_arr, alpha=pc_alpha)
+                        
+                        # Check if any are independent (p-value > alpha)
+                        if bool(jnp.any(pvals > pc_alpha)):
+                            return True
             
             return False
         
@@ -824,35 +1014,60 @@ class PCMCI:
             if len(tests) == 0:
                 continue
 
-            # Prepare batch data
-            X_batch = []
-            Y_batch = []
-            Z_batch = [] if n_cond > 0 else None
-            test_indices = []
-
-            for i, j, tau, cond_set in tests:
-                if cond_set:
-                    X, Y, Z = self.datahandler.get_variable_pair_data(i, j, tau, cond_set)
-                    Z_batch.append(Z)
+            effective_T = self.T - max_lag
+            batch_size = self._get_effective_batch_size(n_samples=effective_T, n_conditions=n_cond)
+            if batch_size is None:
+                chunk_ranges = [(0, len(tests))]
+            else:
+                chunk_ranges = [
+                    (start, min(start + batch_size, len(tests)))
+                    for start in range(0, len(tests), batch_size)
+                ]
+
+            for start, end in chunk_ranges:
+                i_list = []
+                j_list = []
+                tau_list = []
+                cond_vars_list = [] if n_cond > 0 else None
+                cond_lags_list = [] if n_cond > 0 else None
+
+                for i, j, tau, cond_set in tests[start:end]:
+                    i_list.append(i)
+                    j_list.append(j)
+                    tau_list.append(tau)
+                    if n_cond > 0:
+                        cond_vars_list.append([var for var, _ in cond_set])
+                        cond_lags_list.append([lag for _, lag in cond_set])
+
+                i_arr = jnp.asarray(i_list, dtype=jnp.int32)
+                j_arr = jnp.asarray(j_list, dtype=jnp.int32)
+                tau_arr = jnp.asarray(tau_list, dtype=jnp.int32)
+
+                if n_cond > 0:
+                    cond_vars = jnp.asarray(cond_vars_list, dtype=jnp.int32)
+                    cond_lags = jnp.asarray(cond_lags_list, dtype=jnp.int32)
+                    X_arr, Y_arr, Z_arr = self.datahandler.get_variable_pair_batch(
+                        i_arr,
+                        j_arr,
+                        tau_arr,
+                        cond_vars=cond_vars,
+                        cond_lags=cond_lags,
+                        max_lag=max_lag,
+                    )
                 else:
-                    X, Y, Z = self.datahandler.get_variable_pair_data(i, j, tau, None)
-
-                X_batch.append(X)
-                Y_batch.append(Y)
-                test_indices.append((i, j, tau))
-
-            # Stack into arrays
-            X_arr = jnp.stack(X_batch, axis=0)
-            Y_arr = jnp.stack(Y_batch, axis=0)
-            Z_arr = jnp.stack(Z_batch, axis=0) if Z_batch else None
+                    X_arr, Y_arr, Z_arr = self.datahandler.get_variable_pair_batch(
+                        i_arr,
+                        j_arr,
+                        tau_arr,
+                        max_lag=max_lag,
+                    )
 
-            # Run batch test
-            stats, pvals = self.test.run_batch(X_arr, Y_arr, Z_arr)
+                # Run batch test
+                stats, pvals = self.test.run_batch(X_arr, Y_arr, Z_arr)
 
-            # Store results
-            for idx, (i, j, tau) in enumerate(test_indices):
-                val_matrix = val_matrix.at[i, j, tau].set(stats[idx])
-                pval_matrix = pval_matrix.at[i, j, tau].set(pvals[idx])
+                # Store results (vectorized scatter)
+                val_matrix = val_matrix.at[i_arr, j_arr, tau_arr].set(stats)
+                pval_matrix = pval_matrix.at[i_arr, j_arr, tau_arr].set(pvals)
 
         return val_matrix, pval_matrix
 
diff --git a/jax_pcmci/algorithms/pcmci_plus.py b/jax_pcmci/algorithms/pcmci_plus.py
index 1721d35..3024535 100644
--- a/jax_pcmci/algorithms/pcmci_plus.py
+++ b/jax_pcmci/algorithms/pcmci_plus.py
@@ -38,6 +38,7 @@ from typing import Any, Dict, List, Optional, Sequence, Set, Tuple, Union
 
 import jax
 import jax.numpy as jnp
+import numpy as np
 from jax import lax
 from functools import partial
 from itertools import combinations
@@ -169,6 +170,7 @@ class PCMCIPlus(PCMCI):
         max_conds_dim: Optional[int] = None,
         max_conds_py: Optional[int] = None,
         max_conds_px: Optional[int] = None,
+        max_subsets: int = 100,
         alpha_level: float = 0.05,
         fdr_method: Optional[str] = None,
         orientation_alpha: Optional[float] = None,
@@ -197,6 +199,8 @@ class PCMCIPlus(PCMCI):
             Maximum conditions from target's parents.
         max_conds_px : int or None
             Maximum conditions from source's parents.
+        max_subsets : int, default=100
+            Maximum number of conditioning subsets to test per parent in PC phase.
         alpha_level : float, default=0.05
             Final significance level for link discovery.
         fdr_method : str or None
@@ -223,6 +227,9 @@ class PCMCIPlus(PCMCI):
         if orientation_alpha is None:
             orientation_alpha = pc_alpha
 
+        # Precompute lagged data to avoid repeated construction
+        self.datahandler.precompute_lagged_data(tau_max)
+
         if self.verbosity >= 1:
             print(f"\n{'='*60}")
             print("PCMCI+: Contemporaneous and Lagged Causal Discovery")
@@ -241,6 +248,7 @@ class PCMCIPlus(PCMCI):
             tau_min=tau_min,
             pc_alpha=pc_alpha,
             max_conds_dim=max_conds_dim,
+            max_subsets=max_subsets,
         )
 
         # Phase 2: Orientation
@@ -294,6 +302,7 @@ class PCMCIPlus(PCMCI):
         tau_min: int,
         pc_alpha: float,
         max_conds_dim: Optional[int],
+        max_subsets: int = 100,
     ) -> Tuple[Dict[int, Set[Tuple[int, int]]], Dict]:
         """
         Discover the skeleton (undirected graph) using PC-stable.
@@ -330,40 +339,167 @@ class PCMCIPlus(PCMCI):
             skeleton_snapshot = {j: skeleton[j].copy() for j in self.selected_variables}
             any_removed = False
 
-            for j in self.selected_variables:
-                current_adj = list(skeleton_snapshot[j])
-
-                if len(current_adj) <= cond_dim:
-                    continue
-
-                edges_to_remove = []
-
-                for adj in current_adj:
-                    i, neg_tau = adj
-                    tau = -neg_tau
-
-                    # Get other adjacent nodes as potential conditioning set
-                    other_adj = [a for a in current_adj if a != adj]
-
-                    # Test with subsets of size cond_dim
-                    independent, sep_set = self._test_independence_with_subsets(
-                        i=i,
-                        j=j,
-                        tau=tau,
-                        other_adj=other_adj,
-                        cond_dim=cond_dim,
-                        pc_alpha=pc_alpha,
-                    )
-
-                    if independent:
-                        edges_to_remove.append(adj)
-                        # Store separating set
-                        sepsets[(i, j, tau)] = sep_set
-                        sepsets[(j, i, -tau)] = sep_set  # Symmetric
-                        any_removed = True
-
-                for edge in edges_to_remove:
-                    skeleton[j].discard(edge)
+            # For cond_dim=0, use batched testing if available
+            if cond_dim == 0 and hasattr(self.test, 'run_batch'):
+                # Batch test all edges at once grouped by lag
+                for j in self.selected_variables:
+                    current_adj = list(skeleton_snapshot[j])
+                    if not current_adj:
+                        continue
+                    
+                    # Group by lag for proper batching
+                    edges_by_lag: Dict[int, List[Tuple[int, int]]] = {}
+                    for adj in current_adj:
+                        i, neg_tau = adj
+                        tau = -neg_tau
+                        if tau not in edges_by_lag:
+                            edges_by_lag[tau] = []
+                        edges_by_lag[tau].append(adj)
+                    
+                    edges_to_remove = []
+                    
+                    for tau, edges_at_tau in edges_by_lag.items():
+                        i_list = [adj[0] for adj in edges_at_tau]
+                        i_arr = jnp.asarray(i_list, dtype=jnp.int32)
+                        j_arr = jnp.full((len(edges_at_tau),), j, dtype=jnp.int32)
+                        tau_arr = jnp.full((len(edges_at_tau),), tau, dtype=jnp.int32)
+                        
+                        X_batch, Y_batch, _ = self.datahandler.get_variable_pair_batch(
+                            i_arr, j_arr, tau_arr, max_lag=tau
+                        )
+                        
+                        stats, pvals = self.test.run_batch(X_batch, Y_batch, None, alpha=pc_alpha)
+                        
+                        # Vectorized: find independent edges (pvalue > alpha)
+                        independent_mask = np.asarray(pvals > pc_alpha)
+                        for idx in np.where(independent_mask)[0]:
+                            edges_to_remove.append(edges_at_tau[idx])
+                            sepsets[(edges_at_tau[idx][0], j, tau)] = set()
+                            sepsets[(j, edges_at_tau[idx][0], -tau)] = set()
+                            any_removed = True
+                    
+                    for edge in edges_to_remove:
+                        skeleton[j].discard(edge)
+            else:
+                # Cond_dim > 0: Optimize with batching if available
+                if hasattr(self.test, 'run_batch'):
+                    specs_by_lag = {}
+
+                    for j in self.selected_variables:
+                        current_adj = list(skeleton_snapshot[j])
+                        if len(current_adj) <= cond_dim:
+                            continue
+
+                        for adj in current_adj:
+                            i, neg_tau = adj
+                            tau = -neg_tau
+                            other_adj = [a for a in current_adj if a != adj]
+
+                            if len(other_adj) < cond_dim:
+                                continue
+
+                            # Test with subsets of size cond_dim
+                            # Use reservoir sampling to limit combinatorial explosion
+                            subsets_to_test = self._sample_condition_subsets(
+                                other_adj,
+                                cond_dim,
+                                max_subsets,
+                                seed=i * 1000 + j * 100 + tau + cond_dim,
+                            )
+                            
+                            for subset in subsets_to_test:
+                                max_lag_in_subset = max(-p[1] for p in subset) if subset else 0
+                                effective_max_lag = max(tau, max_lag_in_subset)
+                                if effective_max_lag not in specs_by_lag:
+                                    specs_by_lag[effective_max_lag] = []
+                                specs_by_lag[effective_max_lag].append((j, adj, tau, subset))
+
+                    # Process batches grouped by max_lag
+                    for max_lag, specs in specs_by_lag.items():
+                        effective_T = self.T - max_lag
+                        batch_size = self._get_effective_batch_size(
+                            n_samples=effective_T, n_conditions=cond_dim
+                        )
+
+                        if batch_size is None or batch_size > len(specs):
+                            chunk_ranges = [(0, len(specs))]
+                        else:
+                            chunk_ranges = [
+                                (start, min(start + batch_size, len(specs)))
+                                for start in range(0, len(specs), batch_size)
+                            ]
+
+                        for start, end in chunk_ranges:
+                            batch_specs = specs[start:end]
+                            j_list, adj_edges, tau_list, subset_list = zip(*batch_specs)
+
+                            i_list = [p[0] for p in adj_edges]
+                            i_arr = jnp.asarray(i_list, dtype=jnp.int32)
+                            j_arr = jnp.asarray(j_list, dtype=jnp.int32)
+                            tau_arr = jnp.asarray(tau_list, dtype=jnp.int32)
+
+                            # Handle subsets -> numpy for speed
+                            subset_arr = np.array(subset_list, dtype=np.int32)
+                            cond_vars = jnp.array(subset_arr[:, :, 0], dtype=jnp.int32)
+                            cond_lags = jnp.array(-subset_arr[:, :, 1], dtype=jnp.int32)
+
+                            X_b, Y_b, Z_b = self.datahandler.get_variable_pair_batch(
+                                i_arr, j_arr, tau_arr, cond_vars, cond_lags, max_lag=max_lag
+                            )
+
+                            stats, pvals = self.test.run_batch(X_b, Y_b, Z_b, alpha=pc_alpha)
+
+                            # Vectorized: find independent pairs (pvalue > alpha)
+                            independent_mask = np.asarray(pvals > pc_alpha)
+                            for idx in np.where(independent_mask)[0]:
+                                j_idx = j_list[idx]
+                                adj_edge = adj_edges[idx]
+                                tau_val = tau_list[idx]
+                                subset_val = subset_list[idx]
+
+                                if adj_edge in skeleton[j_idx]:
+                                    skeleton[j_idx].discard(adj_edge)
+                                    # Store separating set
+                                    sepsets[(adj_edge[0], j_idx, tau_val)] = set(subset_val)
+                                    sepsets[(j_idx, adj_edge[0], -tau_val)] = set(subset_val)
+                                    any_removed = True
+                else:
+                    # Original sequential code path fallback
+                    for j in self.selected_variables:
+                        current_adj = list(skeleton_snapshot[j])
+
+                        if len(current_adj) <= cond_dim:
+                            continue
+
+                        edges_to_remove = []
+
+                        for adj in current_adj:
+                            i, neg_tau = adj
+                            tau = -neg_tau
+
+                            # Get other adjacent nodes as potential conditioning set
+                            other_adj = [a for a in current_adj if a != adj]
+
+                            # Test with subsets of size cond_dim
+                            independent, sep_set = self._test_independence_with_subsets(
+                                i=i,
+                                j=j,
+                                tau=tau,
+                                other_adj=other_adj,
+                                cond_dim=cond_dim,
+                                pc_alpha=pc_alpha,
+                                max_subsets=max_subsets,
+                            )
+
+                            if independent:
+                                edges_to_remove.append(adj)
+                                # Store separating set
+                                sepsets[(i, j, tau)] = sep_set
+                                sepsets[(j, i, -tau)] = sep_set  # Symmetric
+                                any_removed = True
+
+                        for edge in edges_to_remove:
+                            skeleton[j].discard(edge)
 
             if not any_removed:
                 break
@@ -384,6 +520,7 @@ class PCMCIPlus(PCMCI):
         other_adj: List[Tuple[int, int]],
         cond_dim: int,
         pc_alpha: float,
+        max_subsets: int = 100,
     ) -> Tuple[bool, Set[Tuple[int, int]]]:
         """
         Test independence with conditioning subsets.
@@ -400,7 +537,14 @@ class PCMCIPlus(PCMCI):
                 return True, set()
             return False, set()
 
-        for subset in combinations(other_adj, cond_dim):
+        subsets_to_test = self._sample_condition_subsets(
+            other_adj,
+            cond_dim,
+            max_subsets,
+            seed=i * 1000 + j * 100 + tau,
+        )
+
+        for subset in subsets_to_test:
             # Convert (var, neg_tau) to (var, pos_tau) for data handler
             cond_list = [(var, -neg_tau) for var, neg_tau in subset]
 
@@ -473,8 +617,14 @@ class PCMCIPlus(PCMCI):
 
         If X - Z - Y (X and Y not adjacent) and Z not in sepset(X,Y),
         then X -> Z <- Y.
+        
+        Optimized: Collects all updates and applies them in batch.
         """
         N = self.N
+        
+        # Collect all v-structure updates to batch them
+        updates_arrow = []  # List of (i, j) to set as 2 (arrow)
+        updates_remove = []  # List of (i, j) to set as 0 (remove)
 
         for z in range(N):
             # Find all contemporaneous neighbors of z
@@ -502,15 +652,23 @@ class PCMCIPlus(PCMCI):
 
                         if not z_in_sepset:
                             # Orient as X -> Z <- Y (v-structure)
-                            graph = graph.at[x, z, 0].set(2)  # Arrow from x to z
-                            graph = graph.at[y, z, 0].set(2)  # Arrow from y to z
-                            # Remove circles in opposite direction
-                            graph = graph.at[z, x, 0].set(0)
-                            graph = graph.at[z, y, 0].set(0)
+                            updates_arrow.append((x, z))
+                            updates_arrow.append((y, z))
+                            updates_remove.append((z, x))
+                            updates_remove.append((z, y))
 
                             if self.verbosity >= 2:
                                 print(f"  V-structure: X{x} -> X{z} <- X{y}")
 
+        # Apply all updates using numpy for efficiency (avoid repeated .at[].set())
+        if updates_arrow or updates_remove:
+            graph_np = np.array(graph)
+            for i, j in updates_arrow:
+                graph_np[i, j, 0] = 2
+            for i, j in updates_remove:
+                graph_np[i, j, 0] = 0
+            graph = jnp.array(graph_np)
+
         return graph
 
     def _apply_meek_rules(
@@ -528,40 +686,54 @@ class PCMCIPlus(PCMCI):
         R2: X -> Z -> Y and X - Y  =>  X -> Y
         R3: X - Z -> Y <- W - X  and X - Y  =>  X -> Y
         R4: X - Z -> Y and W -> Z <- X  and X - Y  =>  X -> Y
+        
+        Optimized: Uses numpy for graph operations since N is typically small
+        and avoids creating new JAX arrays in the inner loop.
         """
+        # Convert to numpy for faster in-place updates (small N)
+        graph_np = np.array(graph)
+        N = self.N
+        
         for iteration in range(max_iterations):
             changed = False
-            new_graph = graph.copy()
-
-            # R1: Chain rule
-            for x in range(self.N):
-                for y in range(self.N):
-                    if graph[x, y, 0] == 2:  # X -> Y
-                        for z in range(self.N):
-                            if graph[y, z, 0] == 3:  # Y - Z (undirected)
+
+            # R1: Chain rule - vectorized check
+            # Find all (x, y) where X -> Y (graph[x,y,0] == 2)
+            directed_xy = (graph_np[:, :, 0] == 2)
+            # Find all (y, z) where Y - Z (graph[y,z,0] == 3)
+            undirected_yz = (graph_np[:, :, 0] == 3)
+            
+            for x in range(N):
+                for y in range(N):
+                    if directed_xy[x, y]:
+                        for z in range(N):
+                            if undirected_yz[y, z]:
                                 # Check X not adjacent to Z
-                                x_adj_z = graph[x, z, 0] != 0 or graph[z, x, 0] != 0
+                                x_adj_z = graph_np[x, z, 0] != 0 or graph_np[z, x, 0] != 0
                                 if not x_adj_z:
-                                    new_graph = new_graph.at[y, z, 0].set(2)  # Y -> Z
-                                    new_graph = new_graph.at[z, y, 0].set(0)
+                                    graph_np[y, z, 0] = 2  # Y -> Z
+                                    graph_np[z, y, 0] = 0
                                     changed = True
 
             # R2: Acyclicity rule
-            for x in range(self.N):
-                for y in range(self.N):
-                    if graph[x, y, 0] == 3:  # X - Y
-                        for z in range(self.N):
-                            if graph[x, z, 0] == 2 and graph[z, y, 0] == 2:  # X -> Z -> Y
-                                new_graph = new_graph.at[x, y, 0].set(2)  # X -> Y
-                                new_graph = new_graph.at[y, x, 0].set(0)
+            undirected_xy = (graph_np[:, :, 0] == 3)
+            directed = (graph_np[:, :, 0] == 2)
+            
+            for x in range(N):
+                for y in range(N):
+                    if undirected_xy[x, y]:
+                        # Check if exists z such that X -> Z -> Y
+                        for z in range(N):
+                            if directed[x, z] and directed[z, y]:
+                                graph_np[x, y, 0] = 2  # X -> Y
+                                graph_np[y, x, 0] = 0
                                 changed = True
-
-            graph = new_graph
+                                break  # No need to check more z
 
             if not changed:
                 break
 
-        return graph
+        return jnp.array(graph_np)
 
     def _run_mci_plus(
         self,
@@ -586,35 +758,121 @@ class PCMCIPlus(PCMCI):
                     if oriented_graph[i, j, tau]:
                         parents[j].add((i, -tau))
 
-        # Run MCI tests
-        tests = []
-        for j in self.selected_variables:
-            for i in range(self.N):
-                for tau in range(tau_min, tau_max + 1):
-                    if tau == 0 and i == j:
-                        continue
-                    tests.append((i, j, tau))
-
-        if self.verbosity >= 1:
-            tests = tqdm(tests, desc="MCI+ tests", leave=False)
+        # Run MCI tests (batched when available)
+        if hasattr(self.test, "run_batch"):
+            tests_by_shape: Dict[Tuple[int, int], List[Tuple]] = {}
 
-        for i, j, tau in tests:
-            # Get conditioning set
-            cond_set = self._get_mci_conditions(
-                i, j, tau, parents, max_conds_py, max_conds_px
-            )
-
-            # Run test
-            if cond_set:
-                X, Y, Z = self.datahandler.get_variable_pair_data(i, j, tau, cond_set)
+            for j in self.selected_variables:
+                for i in range(self.N):
+                    for tau in range(tau_min, tau_max + 1):
+                        if tau == 0 and i == j:
+                            continue
+
+                        cond_set = self._get_mci_conditions(
+                            i, j, tau, parents, max_conds_py, max_conds_px
+                        )
+                        n_cond = len(cond_set)
+
+                        if cond_set:
+                            max_cond_lag = max(lag for _, lag in cond_set)
+                            effective_max_lag = max(tau, max_cond_lag)
+                        else:
+                            effective_max_lag = tau
+
+                        key = (n_cond, effective_max_lag)
+                        if key not in tests_by_shape:
+                            tests_by_shape[key] = []
+                        tests_by_shape[key].append((i, j, tau, cond_set))
+
+            if self.verbosity >= 1:
+                iterable = tqdm(tests_by_shape.items(), desc="MCI+ batches", leave=False)
             else:
-                X, Y, Z = self.datahandler.get_variable_pair_data(i, j, tau, None)
-                Z = None
+                iterable = tests_by_shape.items()
 
-            result = self.test.run(X, Y, Z)
+            for (n_cond, max_lag), tests in iterable:
+                if len(tests) == 0:
+                    continue
 
-            val_matrix = val_matrix.at[i, j, tau].set(result.statistic)
-            pval_matrix = pval_matrix.at[i, j, tau].set(result.pvalue)
+                effective_T = self.T - max_lag
+                batch_size = self._get_effective_batch_size(
+                    n_samples=effective_T, n_conditions=n_cond
+                )
+                if batch_size is None:
+                    chunk_ranges = [(0, len(tests))]
+                else:
+                    chunk_ranges = [
+                        (start, min(start + batch_size, len(tests)))
+                        for start in range(0, len(tests), batch_size)
+                    ]
+
+                for start, end in chunk_ranges:
+                    i_list = []
+                    j_list = []
+                    tau_list = []
+                    cond_vars_list = [] if n_cond > 0 else None
+                    cond_lags_list = [] if n_cond > 0 else None
+
+                    for i, j, tau, cond_set in tests[start:end]:
+                        i_list.append(i)
+                        j_list.append(j)
+                        tau_list.append(tau)
+                        if n_cond > 0:
+                            cond_vars_list.append([var for var, _ in cond_set])
+                            cond_lags_list.append([lag for _, lag in cond_set])
+
+                    i_arr = jnp.asarray(i_list, dtype=jnp.int32)
+                    j_arr = jnp.asarray(j_list, dtype=jnp.int32)
+                    tau_arr = jnp.asarray(tau_list, dtype=jnp.int32)
+
+                    if n_cond > 0:
+                        cond_vars = jnp.asarray(cond_vars_list, dtype=jnp.int32)
+                        cond_lags = jnp.asarray(cond_lags_list, dtype=jnp.int32)
+                        X_arr, Y_arr, Z_arr = self.datahandler.get_variable_pair_batch(
+                            i_arr,
+                            j_arr,
+                            tau_arr,
+                            cond_vars=cond_vars,
+                            cond_lags=cond_lags,
+                            max_lag=max_lag,
+                        )
+                    else:
+                        X_arr, Y_arr, Z_arr = self.datahandler.get_variable_pair_batch(
+                            i_arr,
+                            j_arr,
+                            tau_arr,
+                            max_lag=max_lag,
+                        )
+
+                    stats, pvals = self.test.run_batch(X_arr, Y_arr, Z_arr)
+                    val_matrix = val_matrix.at[i_arr, j_arr, tau_arr].set(stats)
+                    pval_matrix = pval_matrix.at[i_arr, j_arr, tau_arr].set(pvals)
+        else:
+            tests = []
+            for j in self.selected_variables:
+                for i in range(self.N):
+                    for tau in range(tau_min, tau_max + 1):
+                        if tau == 0 and i == j:
+                            continue
+                        tests.append((i, j, tau))
+
+            if self.verbosity >= 1:
+                tests = tqdm(tests, desc="MCI+ tests", leave=False)
+
+            for i, j, tau in tests:
+                cond_set = self._get_mci_conditions(
+                    i, j, tau, parents, max_conds_py, max_conds_px
+                )
+
+                if cond_set:
+                    X, Y, Z = self.datahandler.get_variable_pair_data(i, j, tau, cond_set)
+                else:
+                    X, Y, Z = self.datahandler.get_variable_pair_data(i, j, tau, None)
+                    Z = None
+
+                result = self.test.run(X, Y, Z)
+
+                val_matrix = val_matrix.at[i, j, tau].set(result.statistic)
+                pval_matrix = pval_matrix.at[i, j, tau].set(result.pvalue)
 
         return val_matrix, pval_matrix
 
diff --git a/jax_pcmci/config.py b/jax_pcmci/config.py
index 5675ab8..b784179 100644
--- a/jax_pcmci/config.py
+++ b/jax_pcmci/config.py
@@ -97,7 +97,7 @@ class PCMCIConfig:
 
     Parameters
     ----------
-    precision : Precision or str, default='float64'
+    precision : Precision or str, default='float32'
         Numerical precision for computations. Options:
         - 'float32': Faster, less memory, slightly less accurate
         - 'float64': Slower, more memory, more accurate (recommended)
@@ -111,7 +111,7 @@ class PCMCIConfig:
         Random seed for reproducibility. If None, uses system entropy.
     jit_compile : bool, default=True
         Whether to JIT compile core functions. Disable for debugging.
-    enable_x64 : bool, default=True
+    enable_x64 : bool, default=False
         Enable 64-bit floating point support in JAX.
     memory_efficient : bool, default=False
         Enable memory-efficient mode for large datasets.
@@ -125,6 +125,14 @@ class PCMCIConfig:
         Verbosity level (0=silent, 1=normal, 2=verbose, 3=debug).
     cache_results : bool, default=True
         Cache intermediate results to avoid redundant computations.
+    cache_max_entries : int, default=4096
+        Maximum number of cached variable-pair slices.
+    gpu_preallocate : bool, default=True
+        Whether JAX preallocates most GPU memory at startup.
+    gpu_memory_fraction : float or None, default=None
+        Fraction of GPU memory to allocate (e.g., 0.7). None = JAX default.
+    gpu_allocator : str or None, default=None
+        GPU allocator backend, e.g. 'platform' or 'bfc'. None = JAX default.
 
     Examples
     --------
@@ -147,16 +155,20 @@ class PCMCIConfig:
     Use the context manager interface for temporary configuration changes.
     """
 
-    precision: Union[Precision, str] = "float64"
+    precision: Union[Precision, str] = "float32"
     parallelization: Union[ParallelizationMode, str] = "auto"
     random_seed: Optional[int] = None
     jit_compile: bool = True
-    enable_x64: bool = True
+    enable_x64: bool = False
     memory_efficient: bool = False
     batch_size: Optional[int] = None
     progress_bar: bool = True
     verbosity: int = 1
     cache_results: bool = True
+    cache_max_entries: int = 4096
+    gpu_preallocate: bool = True
+    gpu_memory_fraction: Optional[float] = None
+    gpu_allocator: Optional[str] = None
     _previous_config: Optional["PCMCIConfig"] = field(default=None, repr=False)
 
     def __post_init__(self):
@@ -175,6 +187,25 @@ class PCMCIConfig:
         if self.batch_size is not None and self.batch_size < 1:
             raise ValueError(f"batch_size must be positive, got {self.batch_size}")
 
+        if self.cache_max_entries < 1:
+            raise ValueError(
+                f"cache_max_entries must be positive, got {self.cache_max_entries}"
+            )
+
+        if self.gpu_memory_fraction is not None:
+            if not (0.0 < self.gpu_memory_fraction <= 1.0):
+                raise ValueError(
+                    "gpu_memory_fraction must be in (0, 1], got "
+                    f"{self.gpu_memory_fraction}"
+                )
+        if self.gpu_allocator is not None:
+            self.gpu_allocator = self.gpu_allocator.lower()
+            if self.gpu_allocator not in ("platform", "bfc"):
+                raise ValueError(
+                    "gpu_allocator must be 'platform' or 'bfc', got "
+                    f"{self.gpu_allocator}"
+                )
+
     def apply(self) -> None:
         """
         Apply this configuration globally.
@@ -194,6 +225,17 @@ class PCMCIConfig:
         # Keep JAX and our dtype setting consistent to avoid silent truncation.
         jax_config.update("jax_enable_x64", bool(self.enable_x64))
 
+        # Configure GPU memory behavior (must be set before first GPU use).
+        os.environ["XLA_PYTHON_CLIENT_PREALLOCATE"] = (
+            "true" if self.gpu_preallocate else "false"
+        )
+        if self.gpu_memory_fraction is not None:
+            os.environ["XLA_PYTHON_CLIENT_MEM_FRACTION"] = str(
+                self.gpu_memory_fraction
+            )
+        if self.gpu_allocator is not None:
+            os.environ["XLA_PYTHON_CLIENT_ALLOCATOR"] = self.gpu_allocator
+
         if self.verbosity >= 2:
             print(f"Applied configuration: {self}")
 
@@ -381,7 +423,7 @@ def set_device(
         raise ValueError(f"Unknown device: {device}. Use 'cpu', 'gpu', 'tpu', or 'auto'")
 
 
-def get_device_info() -> dict[str, Any]:
+def get_device_info() -> Dict[str, Any]:
     """
     Get information about available JAX devices.
 
diff --git a/jax_pcmci/data.py b/jax_pcmci/data.py
index 5370e1a..ad3b50e 100644
--- a/jax_pcmci/data.py
+++ b/jax_pcmci/data.py
@@ -29,7 +29,9 @@ Example
 from __future__ import annotations
 
 from dataclasses import dataclass, field
-from typing import Any, Callable, Optional, Sequence, Tuple, Union
+from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union
+from collections import OrderedDict
+from functools import partial
 
 import jax
 import jax.numpy as jnp
@@ -85,10 +87,10 @@ class TimeSeriesData:
     """
 
     values: jax.Array
-    var_names: Optional[list[str]] = None
+    var_names: Optional[List[str]] = None
     time_index: Optional[jax.Array] = None
     mask: Optional[jax.Array] = None
-    metadata: dict[str, Any] = field(default_factory=dict)
+    metadata: Dict[str, Any] = field(default_factory=dict)
 
     def __post_init__(self):
         """Validate and initialize data attributes."""
@@ -266,7 +268,7 @@ class DataHandler:
         data: Union[np.ndarray, jax.Array, TimeSeriesData],
         normalize: Union[bool, str] = True,
         missing_flag: Optional[float] = None,
-        var_names: Optional[list[str]] = None,
+        var_names: Optional[List[str]] = None,
         dtype: Optional[jnp.dtype] = None,
     ):
         self._dtype = dtype or get_config().dtype
@@ -281,8 +283,21 @@ class DataHandler:
             data_array = jnp.asarray(data, dtype=self._dtype)
             self._data = TimeSeriesData(values=data_array, var_names=var_names)
 
+        # Get config once
+        config = get_config()
+        
         # Cache for lagged data cubes - keyed by (tau_max, include_contemporaneous)
-        self._lagged_cache: Dict[Tuple[int, bool], Tuple[jax.Array, jax.Array]] = {}
+        # Use OrderedDict for LRU cache behavior
+        self._lagged_cache: "OrderedDict[Tuple[int, bool], Tuple[jax.Array, jax.Array]]" = OrderedDict()
+        self._cache_max_entries = config.cache_max_entries
+
+        # Cache for variable pair slices
+        self._pair_cache: Optional[
+            "OrderedDict[Tuple, Tuple[jax.Array, jax.Array, Optional[jax.Array]]]"
+        ] = (
+            OrderedDict() if config.cache_results else None
+        )
+        self._pair_cache_max = config.cache_max_entries
 
         # Handle missing values
         if missing_flag is not None:
@@ -320,7 +335,7 @@ class DataHandler:
         return self._data.N
 
     @property
-    def var_names(self) -> list[str]:
+    def var_names(self) -> List[str]:
         """Variable names."""
         return self._data.var_names
 
@@ -343,6 +358,10 @@ class DataHandler:
         # Update mask
         self._data.mask = ~missing_mask
 
+        # Replace missing values with NaN to ensure downstream reductions ignore them
+        # (normalization and statistics use nan-aware reductions).
+        self._data.values = jnp.where(self._data.mask, self._data.values, jnp.nan)
+
         # Count missing values
         n_missing = jnp.sum(missing_mask)
         if n_missing > 0 and get_config().verbosity >= 1:
@@ -434,6 +453,8 @@ class DataHandler:
         # Check cache
         cache_key = (tau_max, include_contemporaneous)
         if cache_key in self._lagged_cache:
+            # Move to end (most recently used)
+            self._lagged_cache.move_to_end(cache_key)
             return self._lagged_cache[cache_key]
 
         T, N = self.T, self.N
@@ -442,25 +463,25 @@ class DataHandler:
         # Current values (t = tau_max, tau_max+1, ..., T-1)
         X_current = self.values[tau_max:]
 
-        # Build lagged array using vectorized operations
+        # Build lagged array using simple slicing (faster than lax.scan for this pattern)
         if include_contemporaneous:
-            # Include lag 0, 1, ..., tau_max
-            n_lags = tau_max + 1
-            # Use list comprehension + stack for efficiency
-            lagged_slices = [
-                self.values[tau_max - lag : T - lag if lag > 0 else None]
-                for lag in range(n_lags)
-            ]
-            X_lagged = jnp.stack(lagged_slices, axis=-1)
+            lags = list(range(0, tau_max + 1))
         else:
-            # Only lags 1, 2, ..., tau_max
-            lagged_slices = [
-                self.values[tau_max - lag : T - lag]
-                for lag in range(1, tau_max + 1)
-            ]
-            X_lagged = jnp.stack(lagged_slices, axis=-1)
-
-        # Store in cache
+            lags = list(range(1, tau_max + 1))
+
+        n_lags = len(lags)
+        lagged_slices = []
+        for lag in lags:
+            start_t = tau_max - lag
+            end_t = start_t + effective_T
+            lagged_slices.append(self.values[start_t:end_t, :])
+        
+        X_lagged = jnp.stack(lagged_slices, axis=2)
+
+        # Store in cache with LRU eviction
+        if len(self._lagged_cache) >= self._cache_max_entries:
+            # Remove oldest entry
+            self._lagged_cache.popitem(last=False)
         self._lagged_cache[cache_key] = (X_current, X_lagged)
 
         return X_current, X_lagged
@@ -468,6 +489,8 @@ class DataHandler:
     def clear_cache(self) -> None:
         """Clear the lagged data cache to free memory."""
         self._lagged_cache.clear()
+        if self._pair_cache is not None:
+            self._pair_cache.clear()
 
     def precompute_lagged_data(self, tau_max: int) -> None:
         """
@@ -527,6 +550,16 @@ class DataHandler:
         if tau < 0:
             raise ValueError(f"tau must be non-negative, got {tau}")
 
+        cache_key = None
+        if self._pair_cache is not None:
+            cond_key = None
+            if condition_indices:
+                cond_key = tuple(sorted(condition_indices))
+            cache_key = (i, j, tau, cond_key)
+            if cache_key in self._pair_cache:
+                self._pair_cache.move_to_end(cache_key)
+                return self._pair_cache[cache_key]
+
         # Determine the effective time range
         max_lag = tau
         if condition_indices:
@@ -560,6 +593,90 @@ class DataHandler:
         else:
             Z = None
 
+        if self._pair_cache is not None and cache_key is not None:
+            self._pair_cache[cache_key] = (X, Y, Z)
+            self._pair_cache.move_to_end(cache_key)
+            if len(self._pair_cache) > self._pair_cache_max:
+                self._pair_cache.popitem(last=False)
+
+        return X, Y, Z
+
+    @staticmethod
+    @partial(jax.jit, static_argnums=(3,))
+    def _batch_slice_1d(
+        values: jax.Array,
+        start_idxs: jax.Array,
+        var_idxs: jax.Array,
+        length: int,
+    ) -> jax.Array:
+        """Vectorized 1D slice extraction using dynamic slicing."""
+
+        def slice_one(start_idx, var_idx):
+            return lax.dynamic_slice(values, (start_idx, var_idx), (length, 1)).squeeze(1)
+
+        return jax.vmap(slice_one)(start_idxs, var_idxs)
+
+    def get_variable_pair_batch(
+        self,
+        i_arr: jax.Array,
+        j_arr: jax.Array,
+        tau_arr: jax.Array,
+        cond_vars: Optional[jax.Array] = None,
+        cond_lags: Optional[jax.Array] = None,
+        max_lag: Optional[int] = None,
+    ) -> Tuple[jax.Array, jax.Array, Optional[jax.Array]]:
+        """
+        Get batched data for testing conditional independence across pairs.
+
+        Parameters
+        ----------
+        i_arr : jax.Array
+            Source variable indices, shape (batch,).
+        j_arr : jax.Array
+            Target variable indices, shape (batch,).
+        tau_arr : jax.Array
+            Lags for each pair, shape (batch,).
+        cond_vars : jax.Array, optional
+            Conditioning variable indices, shape (batch, n_cond).
+        cond_lags : jax.Array, optional
+            Conditioning lags, shape (batch, n_cond).
+        max_lag : int, optional
+            Precomputed max lag for alignment.
+        """
+        if max_lag is None:
+            max_lag = int(jnp.max(tau_arr))
+            if cond_lags is not None:
+                max_lag = max(max_lag, int(jnp.max(cond_lags)))
+
+        effective_T = self.T - max_lag
+        if effective_T <= 0:
+            raise ValueError(
+                f"Not enough data points. T={self.T}, max_lag={max_lag}"
+            )
+
+        start_x = max_lag - tau_arr
+        start_y = jnp.full_like(tau_arr, max_lag)
+
+        X = self._batch_slice_1d(self.values, start_x, i_arr, effective_T)
+        Y = self._batch_slice_1d(self.values, start_y, j_arr, effective_T)
+
+        if cond_vars is None or cond_lags is None:
+            return X, Y, None
+
+        n_cond = cond_vars.shape[1]
+        batch_size = cond_vars.shape[0]
+        
+        # Vectorize over conditioning variables: process all at once
+        # Reshape to (batch * n_cond,) for vectorized slicing
+        vars_flat = cond_vars.reshape(-1)
+        lags_flat = cond_lags.reshape(-1)
+        starts_flat = max_lag - lags_flat
+        
+        # Get all conditioning slices in one vectorized call
+        Z_flat = self._batch_slice_1d(self.values, starts_flat, vars_flat, effective_T)
+        
+        # Reshape back to (batch, effective_T, n_cond)
+        Z = Z_flat.reshape(batch_size, n_cond, effective_T).transpose(0, 2, 1)
         return X, Y, Z
 
     @staticmethod
diff --git a/jax_pcmci/independence_tests/__init__.py b/jax_pcmci/independence_tests/__init__.py
index e53d115..e01591a 100644
--- a/jax_pcmci/independence_tests/__init__.py
+++ b/jax_pcmci/independence_tests/__init__.py
@@ -16,12 +16,13 @@ All tests follow a common interface defined by the CondIndTest base class.
 
 from jax_pcmci.independence_tests.base import CondIndTest
 from jax_pcmci.independence_tests.parcorr import ParCorr
-from jax_pcmci.independence_tests.cmi_knn import CMIKnn
+from jax_pcmci.independence_tests.cmi_knn import CMIKnn, CMISymbolic
 from jax_pcmci.independence_tests.gpdc import GPDCond
 
 __all__ = [
     "CondIndTest",
     "ParCorr",
     "CMIKnn",
+    "CMISymbolic",
     "GPDCond",
 ]
diff --git a/jax_pcmci/independence_tests/__pycache__/__init__.cpython-313.pyc b/jax_pcmci/independence_tests/__pycache__/__init__.cpython-313.pyc
index f62c0a3..34a154b 100644
Binary files a/jax_pcmci/independence_tests/__pycache__/__init__.cpython-313.pyc and b/jax_pcmci/independence_tests/__pycache__/__init__.cpython-313.pyc differ
diff --git a/jax_pcmci/independence_tests/__pycache__/base.cpython-313.pyc b/jax_pcmci/independence_tests/__pycache__/base.cpython-313.pyc
index 336648f..8cb1576 100644
Binary files a/jax_pcmci/independence_tests/__pycache__/base.cpython-313.pyc and b/jax_pcmci/independence_tests/__pycache__/base.cpython-313.pyc differ
diff --git a/jax_pcmci/independence_tests/__pycache__/cmi_knn.cpython-313.pyc b/jax_pcmci/independence_tests/__pycache__/cmi_knn.cpython-313.pyc
index 14f6778..e1ddd73 100644
Binary files a/jax_pcmci/independence_tests/__pycache__/cmi_knn.cpython-313.pyc and b/jax_pcmci/independence_tests/__pycache__/cmi_knn.cpython-313.pyc differ
diff --git a/jax_pcmci/independence_tests/__pycache__/gpdc.cpython-313.pyc b/jax_pcmci/independence_tests/__pycache__/gpdc.cpython-313.pyc
index 5bf8bb5..deba03d 100644
Binary files a/jax_pcmci/independence_tests/__pycache__/gpdc.cpython-313.pyc and b/jax_pcmci/independence_tests/__pycache__/gpdc.cpython-313.pyc differ
diff --git a/jax_pcmci/independence_tests/__pycache__/parcorr.cpython-313.pyc b/jax_pcmci/independence_tests/__pycache__/parcorr.cpython-313.pyc
index 5a11a98..ec64568 100644
Binary files a/jax_pcmci/independence_tests/__pycache__/parcorr.cpython-313.pyc and b/jax_pcmci/independence_tests/__pycache__/parcorr.cpython-313.pyc differ
diff --git a/jax_pcmci/independence_tests/base.py b/jax_pcmci/independence_tests/base.py
index f2e24ac..ea9c749 100644
--- a/jax_pcmci/independence_tests/base.py
+++ b/jax_pcmci/independence_tests/base.py
@@ -83,6 +83,9 @@ class TestResult:
     test_name: str
     extra_info: dict = field(default_factory=dict)
 
+    # Prevent pytest from treating this as a test class
+    __test__ = False
+
     def __repr__(self) -> str:
         sig_str = "***" if self.significant else ""
         return (
@@ -176,11 +179,9 @@ class CondIndTest(ABC):
         self._base_key = jax.random.PRNGKey(seed)
         self._rng_counter = 0
 
-        # Cached JITed batch runners to avoid Python overhead on repeated calls
-        self._run_batch_no_z = jax.jit(lambda X, Y: self._run_batch_no_z_impl(X, Y))
-        self._run_batch_with_z = jax.jit(
-            lambda X, Y, Z: self._run_batch_with_z_impl(X, Y, Z)
-        )
+        # Note: We don't cache JIT'd batch runners at instance level because
+        # capturing `self` in a JIT'd lambda causes recompilation per instance.
+        # Instead, subclasses should override run_batch with proper JIT strategy.
 
     @abstractmethod
     def compute_statistic(
@@ -359,11 +360,11 @@ class CondIndTest(ABC):
             Batch of conditioning variables, shape (n_tests, n_samples, n_cond).
         alpha : float or None
         """
+        # Call implementations directly - subclasses like ParCorr override
+        # run_batch for better JIT strategy
         if Z_batch is None:
-            return self._run_batch_no_z(X_batch, Y_batch)
-        return self._run_batch_with_z(X_batch, Y_batch, Z_batch)
-
-        return stats, pvals
+            return self._run_batch_no_z_impl(X_batch, Y_batch)
+        return self._run_batch_with_z_impl(X_batch, Y_batch, Z_batch)
 
     # ----- JITed batch helpers -------------------------------------------------
 
diff --git a/jax_pcmci/independence_tests/cmi_knn.py b/jax_pcmci/independence_tests/cmi_knn.py
index fc5b846..5992aad 100644
--- a/jax_pcmci/independence_tests/cmi_knn.py
+++ b/jax_pcmci/independence_tests/cmi_knn.py
@@ -171,18 +171,12 @@ class CMIKnn(CondIndTest):
         jax.Array
             CMI estimate (non-negative scalar).
         """
-        dtype = get_config().dtype
-        X = jnp.asarray(X, dtype=dtype).reshape(-1, 1)
-        Y = jnp.asarray(Y, dtype=dtype).reshape(-1, 1)
-
+        # Use _prepare_inputs which handles dtype conversion, reshaping, and standardization
         if Z is None:
             X_prep, Y_prep, _ = self._prepare_inputs(X, Y, None)
             return self._compute_mi_standardized(X_prep, Y_prep)
 
-        Z_arr = jnp.asarray(Z, dtype=dtype)
-        if Z_arr.ndim == 1:
-            Z_arr = Z_arr.reshape(-1, 1)
-        X_prep, Y_prep, Z_prep = self._prepare_inputs(X, Y, Z_arr)
+        X_prep, Y_prep, Z_prep = self._prepare_inputs(X, Y, Z)
         return self._compute_cmi_standardized(X_prep, Y_prep, Z_prep)
 
     def _compute_mi_standardized(self, X: jax.Array, Y: jax.Array) -> jax.Array:
@@ -197,12 +191,15 @@ class CMIKnn(CondIndTest):
         # Joint space XY
         XY = jnp.concatenate([X, Y], axis=1)
 
-        # Find k-th neighbor distance in joint space
-        eps = self._kth_neighbor_distance(XY, k)
+        # Compute distances once for joint space and reuse for counting
+        dist_XY = self._compute_distances(XY)
+        eps = self._kth_neighbor_from_dist(dist_XY, k)
 
-        # Count neighbors within eps in marginal spaces
-        n_X = self._count_neighbors(X, eps)
-        n_Y = self._count_neighbors(Y, eps)
+        # Count neighbors using precomputed marginal distances
+        dist_X = self._compute_distances(X)
+        dist_Y = self._compute_distances(Y)
+        n_X = self._count_neighbors_from_dist(dist_X, eps)
+        n_Y = self._count_neighbors_from_dist(dist_Y, eps)
 
         # KSG estimator
         mi = digamma(k) - jnp.mean(digamma(n_X + 1) + digamma(n_Y + 1)) + digamma(n)
@@ -218,22 +215,42 @@ class CMIKnn(CondIndTest):
         Inputs are assumed to be pre-processed (dtype/shape/optional standardization).
         Uses the Frenzel-Pompe estimator:
             I(X; Y | Z) = Ïˆ(k) - <Ïˆ(n_XZ + 1) + Ïˆ(n_YZ + 1) - Ïˆ(n_Z + 1)>
+        
+        Optimized: For Chebyshev metric, joint distances are max of component distances.
+        We compute X, Y, Z distances once and combine them efficiently.
         """
         n = X.shape[0]
         k = min(self.k, n - 1)
 
-        # Build spaces
-        XZ = jnp.concatenate([X, Z], axis=1)
-        YZ = jnp.concatenate([Y, Z], axis=1)
-        XYZ = jnp.concatenate([X, Y, Z], axis=1)
-
-        # Find k-th neighbor distance in joint space XYZ
-        eps = self._kth_neighbor_distance(XYZ, k)
-
-        # Count neighbors within eps in subspaces
-        n_XZ = self._count_neighbors(XZ, eps)
-        n_YZ = self._count_neighbors(YZ, eps)
-        n_Z = self._count_neighbors(Z, eps)
+        # For Chebyshev metric, d(XYZ) = max(d(X), d(Y), d(Z))
+        # This allows us to compute component distances once and reuse
+        if self.metric == "chebyshev":
+            # Compute component distances once
+            dist_X = self._chebyshev_distances(X, X)
+            dist_Y = self._chebyshev_distances(Y, Y)
+            dist_Z = self._chebyshev_distances(Z, Z)
+            
+            # Joint distances via max (Chebyshev property)
+            dist_XZ = jnp.maximum(dist_X, dist_Z)
+            dist_YZ = jnp.maximum(dist_Y, dist_Z)
+            dist_XYZ = jnp.maximum(dist_XZ, dist_Y)  # = max(X, Y, Z)
+        else:
+            # Euclidean: need to compute full joint spaces
+            XZ = jnp.concatenate([X, Z], axis=1)
+            YZ = jnp.concatenate([Y, Z], axis=1)
+            XYZ = jnp.concatenate([X, Y, Z], axis=1)
+            dist_XZ = self._euclidean_distances(XZ, XZ)
+            dist_YZ = self._euclidean_distances(YZ, YZ)
+            dist_XYZ = self._euclidean_distances(XYZ, XYZ)
+            dist_Z = self._euclidean_distances(Z, Z)
+
+        # Find eps from joint space XYZ
+        eps = self._kth_neighbor_from_dist(dist_XYZ, k)
+
+        # Count neighbors in subspaces
+        n_XZ = self._count_neighbors_from_dist(dist_XZ, eps)
+        n_YZ = self._count_neighbors_from_dist(dist_YZ, eps)
+        n_Z = self._count_neighbors_from_dist(dist_Z, eps)
 
         # Frenzel-Pompe CMI estimator
         cmi = (
@@ -243,47 +260,49 @@ class CMIKnn(CondIndTest):
 
         return jnp.maximum(cmi, 0.0)
 
-    def _kth_neighbor_distance(self, data: jax.Array, k: int) -> jax.Array:
+    def _compute_distances(self, data: jax.Array) -> jax.Array:
         """
-        Find the distance to the k-th nearest neighbor for each point.
+        Compute pairwise distances for a dataset.
         """
-        n = data.shape[0]
-
-        # Compute pairwise distances
         if self.metric == "chebyshev":
-            # Maximum norm: max|x_i - y_i|
-            distances = self._chebyshev_distances(data, data)
+            return self._chebyshev_distances(data, data)
         else:
-            # Euclidean norm
-            distances = self._euclidean_distances(data, data)
+            return self._euclidean_distances(data, data)
 
+    def _kth_neighbor_from_dist(self, distances: jax.Array, k: int) -> jax.Array:
+        """
+        Find the k-th nearest neighbor distance from precomputed distance matrix.
+        """
+        n = distances.shape[0]
         # Set self-distance to infinity
-        distances = distances + jnp.eye(n) * jnp.inf
-
+        distances_masked = distances + jnp.eye(n) * jnp.inf
         # Get k-th neighbor without a full sort (much faster than sort)
-        # (0-indexed, so k-1)
-        kth_distances = jnp.partition(distances, k - 1, axis=1)[:, k - 1]
-
+        kth_distances = jnp.partition(distances_masked, k - 1, axis=1)[:, k - 1]
         return kth_distances
 
-    def _count_neighbors(self, data: jax.Array, eps: jax.Array) -> jax.Array:
+    def _count_neighbors_from_dist(self, distances: jax.Array, eps: jax.Array) -> jax.Array:
         """
-        Count number of points within distance eps for each point.
+        Count neighbors within eps distance from precomputed distance matrix.
         """
-        n = data.shape[0]
-
-        # Compute pairwise distances
-        if self.metric == "chebyshev":
-            distances = self._chebyshev_distances(data, data)
-        else:
-            distances = self._euclidean_distances(data, data)
-
-        # Count points strictly within eps (excluding self)
-        # Using < instead of <= as in standard KSG
+        # Count points strictly within eps (excluding self where distance == 0)
         within_eps = (distances < eps.reshape(-1, 1)) & (distances > 0)
-        counts = jnp.sum(within_eps, axis=1)
+        return jnp.sum(within_eps, axis=1)
+
+    def _kth_neighbor_distance(self, data: jax.Array, k: int) -> jax.Array:
+        """
+        Find the distance to the k-th nearest neighbor for each point.
+        (Legacy method - kept for compatibility)
+        """
+        distances = self._compute_distances(data)
+        return self._kth_neighbor_from_dist(distances, k)
 
-        return counts
+    def _count_neighbors(self, data: jax.Array, eps: jax.Array) -> jax.Array:
+        """
+        Count number of points within distance eps for each point.
+        (Legacy method - kept for compatibility)
+        """
+        distances = self._compute_distances(data)
+        return self._count_neighbors_from_dist(distances, eps)
 
     @staticmethod
     @jax.jit
@@ -551,10 +570,11 @@ class CMISymbolic(CondIndTest):
         if Z.ndim == 1:
             Z_enc = Z
         else:
-            # Simple encoding: treat as base-n_symbols number
-            Z_enc = jnp.zeros(n, dtype=jnp.int32)
-            for i in range(Z.shape[1]):
-                Z_enc = Z_enc * self.n_symbols + Z[:, i].astype(jnp.int32)
+            # Simple encoding: treat as base-n_symbols number using vectorized reduction
+            # Z_enc = Z[:, 0] * n_symbols^(d-1) + Z[:, 1] * n_symbols^(d-2) + ... + Z[:, d-1]
+            d = Z.shape[1]
+            powers = jnp.power(self.n_symbols, jnp.arange(d - 1, -1, -1))
+            Z_enc = jnp.sum(Z.astype(jnp.int32) * powers, axis=1)
 
         # Compute conditional entropies
         H_X_given_Z = self._conditional_entropy(X, Z_enc)
diff --git a/jax_pcmci/independence_tests/parcorr.py b/jax_pcmci/independence_tests/parcorr.py
index 3e0be15..7e30840 100644
--- a/jax_pcmci/independence_tests/parcorr.py
+++ b/jax_pcmci/independence_tests/parcorr.py
@@ -34,17 +34,158 @@ Example
 from __future__ import annotations
 
 from functools import partial
-from typing import Optional
+from typing import Optional, Tuple
 
 import jax
 import jax.numpy as jnp
 from jax import lax
 from jax.scipy import stats as jax_stats
+from jax.scipy import linalg
 
 from jax_pcmci.independence_tests.base import CondIndTest
 from jax_pcmci.config import get_config
 
 
+# ============================================================================
+# Standalone JIT-compiled functions for maximum performance
+# These avoid the overhead of `self` in static_argnums
+# ============================================================================
+
+
+@jax.jit
+def _parcorr_pvalue(statistic: jax.Array, n_samples: int, n_conditions: int) -> jax.Array:
+    """
+    Compute p-value using Fisher's z-transformation (standalone JITted version).
+    
+    This is a module-level function to avoid JIT recompilation when `self` changes.
+    """
+    # Degrees of freedom
+    df = n_samples - n_conditions - 3
+
+    # Handle edge cases
+    df = jnp.maximum(df, 1)
+
+    # Fisher's z-transformation
+    # Clip to avoid log(0) for |r| = 1
+    r_clipped = jnp.clip(statistic, -0.9999, 0.9999)
+    z = 0.5 * jnp.log((1 + r_clipped) / (1 - r_clipped))
+
+    # Standard error under H0
+    se = 1.0 / jnp.sqrt(df)
+
+    # Test statistic
+    z_stat = z / se
+
+    # Two-sided p-value from standard normal
+    pvalue = 2 * (1 - jax_stats.norm.cdf(jnp.abs(z_stat)))
+
+    return pvalue
+
+
+@jax.jit
+def _compute_correlation_jit(X: jax.Array, Y: jax.Array) -> jax.Array:
+    """
+    Compute Pearson correlation between X and Y (standalone JITted version).
+    """
+    # Center the variables
+    X_centered = X - jnp.mean(X)
+    Y_centered = Y - jnp.mean(Y)
+
+    # Compute correlation
+    numerator = jnp.sum(X_centered * Y_centered)
+    denominator = jnp.sqrt(jnp.sum(X_centered**2) * jnp.sum(Y_centered**2))
+
+    # Handle zero denominator
+    correlation = jnp.where(
+        denominator > 1e-10,
+        numerator / denominator,
+        0.0
+    )
+
+    # Clip to [-1, 1] for numerical stability
+    return jnp.clip(correlation, -1.0, 1.0)
+
+
+@jax.jit
+def _compute_partial_correlation_jit(X: jax.Array, Y: jax.Array, Z: jax.Array) -> jax.Array:
+    """
+    Compute partial correlation via OLS residuals (standalone JITted version).
+    Optimized to reuse Cholesky factorization for X and Y regressions.
+    """
+    # Ensure Z is 2D
+    Z = jnp.atleast_2d(Z)
+    if Z.shape[0] == 1 and Z.shape[1] != X.shape[0]:
+        Z = Z.T
+        
+    # Center data
+    Z_c = Z - jnp.mean(Z, axis=0)
+    X_c = X - jnp.mean(X)
+    Y_c = Y - jnp.mean(Y)
+    
+    # Solve normal equations: (Z^T Z) beta = Z^T target
+    # We factorize Z^T Z once and use it for both X and Y
+    gram = Z_c.T @ Z_c
+    ridge = 1e-6 * jnp.eye(gram.shape[0], dtype=gram.dtype)
+    
+    # Cholesky factorization
+    # (c, lower) is the format used by cho_solve
+    # c is the factor, lower=True means L in LL^T
+    L_and_lower = linalg.cho_factor(gram + ridge, lower=True)
+    
+    # Regress X on Z
+    rhs_X = Z_c.T @ X_c
+    coeffs_X = linalg.cho_solve(L_and_lower, rhs_X)
+    rX = X_c - Z_c @ coeffs_X
+    
+    # Regress Y on Z
+    rhs_Y = Z_c.T @ Y_c
+    coeffs_Y = linalg.cho_solve(L_and_lower, rhs_Y)
+    rY = Y_c - Z_c @ coeffs_Y
+    
+    # Compute correlation of centered residuals
+    num = jnp.sum(rX * rY)
+    den = jnp.sqrt(jnp.sum(rX**2) * jnp.sum(rY**2))
+    
+    correlation = jnp.where(
+        den > 1e-10,
+        num / den,
+        0.0
+    )
+    
+    return jnp.clip(correlation, -1.0, 1.0)
+
+
+# Vectorized batch versions for maximum GPU throughput
+@jax.jit
+def _batch_correlation_jit(X_batch: jax.Array, Y_batch: jax.Array) -> jax.Array:
+    """Compute correlations for batched inputs."""
+    return jax.vmap(_compute_correlation_jit)(X_batch, Y_batch)
+
+
+@jax.jit
+def _batch_partial_correlation_jit(
+    X_batch: jax.Array, Y_batch: jax.Array, Z_batch: jax.Array
+) -> jax.Array:
+    """Compute partial correlations for batched inputs."""
+    return jax.vmap(_compute_partial_correlation_jit)(X_batch, Y_batch, Z_batch)
+
+
+@jax.jit
+def _batch_pvalue_jit(
+    statistics: jax.Array, n_samples: jax.Array, n_conditions: jax.Array
+) -> jax.Array:
+    """Compute p-values for batched statistics (vectorized)."""
+    df = n_samples - n_conditions - 3
+    df = jnp.maximum(df, 1)
+
+    r_clipped = jnp.clip(statistics, -0.9999, 0.9999)
+    z = 0.5 * jnp.log((1 + r_clipped) / (1 - r_clipped))
+    se = 1.0 / jnp.sqrt(df)
+    z_stat = z / se
+
+    return 2 * (1 - jax_stats.norm.cdf(jnp.abs(z_stat)))
+
+
 class ParCorr(CondIndTest):
     """
     Partial Correlation test for linear conditional independence.
@@ -132,7 +273,6 @@ class ParCorr(CondIndTest):
         )
         self.robust = robust
 
-    @partial(jax.jit, static_argnums=(0,))
     def compute_statistic(
         self, X: jax.Array, Y: jax.Array, Z: Optional[jax.Array] = None
     ) -> jax.Array:
@@ -153,94 +293,22 @@ class ParCorr(CondIndTest):
         jax.Array
             Partial correlation coefficient in [-1, 1].
         """
-        # Ensure proper dtype
+        # Ensure proper dtype - only convert if needed
         dtype = get_config().dtype
-        X = jnp.asarray(X, dtype=dtype)
-        Y = jnp.asarray(Y, dtype=dtype)
+        if not isinstance(X, jax.Array) or X.dtype != dtype:
+            X = jnp.asarray(X, dtype=dtype)
+        if not isinstance(Y, jax.Array) or Y.dtype != dtype:
+            Y = jnp.asarray(Y, dtype=dtype)
 
         if Z is None or (hasattr(Z, 'shape') and Z.shape[-1] == 0):
             # Simple correlation (no conditioning)
-            return self._compute_correlation(X, Y)
+            return _compute_correlation_jit(X, Y)
         else:
-            Z = jnp.asarray(Z, dtype=dtype)
+            if not isinstance(Z, jax.Array) or Z.dtype != dtype:
+                Z = jnp.asarray(Z, dtype=dtype)
             # Partial correlation via residuals
-            return self._compute_partial_correlation(X, Y, Z)
-
-    @staticmethod
-    @jax.jit
-    def _compute_correlation(X: jax.Array, Y: jax.Array) -> jax.Array:
-        """
-        Compute Pearson correlation between X and Y.
-
-        Uses a numerically stable computation via centered and normalized
-        vectors.
-        """
-        # Center the variables
-        X_centered = X - jnp.mean(X)
-        Y_centered = Y - jnp.mean(Y)
-
-        # Compute correlation
-        numerator = jnp.sum(X_centered * Y_centered)
-        denominator = jnp.sqrt(jnp.sum(X_centered**2) * jnp.sum(Y_centered**2))
-
-        # Handle zero denominator
-        correlation = jnp.where(
-            denominator > 1e-10,
-            numerator / denominator,
-            0.0
-        )
-
-        # Clip to [-1, 1] for numerical stability
-        return jnp.clip(correlation, -1.0, 1.0)
-
-    @staticmethod
-    @jax.jit
-    def _compute_partial_correlation(
-        X: jax.Array, Y: jax.Array, Z: jax.Array
-    ) -> jax.Array:
-        """
-        Compute partial correlation via OLS residuals.
-
-        This computes the correlation between the residuals of X and Y
-        after regressing each on Z.
-        """
-        # Get residuals from regressing X on Z
-        X_residual = ParCorr._compute_residual(X, Z)
-
-        # Get residuals from regressing Y on Z
-        Y_residual = ParCorr._compute_residual(Y, Z)
-
-        # Correlation of residuals
-        return ParCorr._compute_correlation(X_residual, Y_residual)
-
-    @staticmethod
-    @jax.jit
-    def _compute_residual(target: jax.Array, predictors: jax.Array) -> jax.Array:
-        """
-        Compute OLS residuals: target - Z @ (Z^T Z)^{-1} Z^T target
-
-        Uses the pseudoinverse for numerical stability.
-        """
-        # Ensure 2D
-        if predictors.ndim == 1:
-            predictors = predictors.reshape(-1, 1)
-
-        # Add intercept
-        n = len(target)
-        ones = jnp.ones((n, 1), dtype=predictors.dtype)
-        Z_with_intercept = jnp.concatenate([ones, predictors], axis=1)
-
-        # Solve least squares using pseudoinverse
-        # coeffs = (Z^T Z)^{-1} Z^T y
-        coeffs = jnp.linalg.lstsq(Z_with_intercept, target, rcond=None)[0]
-
-        # Compute residuals
-        predicted = Z_with_intercept @ coeffs
-        residual = target - predicted
+            return _compute_partial_correlation_jit(X, Y, Z)
 
-        return residual
-
-    @partial(jax.jit, static_argnums=(0,))
     def compute_pvalue(
         self, statistic: jax.Array, n_samples: int, n_conditions: int
     ) -> jax.Array:
@@ -270,27 +338,7 @@ class ParCorr(CondIndTest):
         jax.Array
             Two-sided p-value.
         """
-        # Degrees of freedom
-        df = n_samples - n_conditions - 3
-
-        # Handle edge cases
-        df = jnp.maximum(df, 1)
-
-        # Fisher's z-transformation
-        # Clip to avoid log(0) for |r| = 1
-        r_clipped = jnp.clip(statistic, -0.9999, 0.9999)
-        z = 0.5 * jnp.log((1 + r_clipped) / (1 - r_clipped))
-
-        # Standard error under H0
-        se = 1.0 / jnp.sqrt(df)
-
-        # Test statistic
-        z_stat = z / se
-
-        # Two-sided p-value from standard normal
-        pvalue = 2 * (1 - jax_stats.norm.cdf(jnp.abs(z_stat)))
-
-        return pvalue
+        return _parcorr_pvalue(statistic, n_samples, n_conditions)
 
     def get_correlation_matrix(
         self, data: jax.Array, tau_max: int = 0
@@ -323,26 +371,46 @@ class ParCorr(CondIndTest):
         >>> print(f"Corr(X0(t-2), X1(t)): {corr_matrix[0, 1, 2]:.3f}")
         """
         T, N = data.shape
-        corr_matrix = jnp.zeros((N, N, tau_max + 1))
-
-        for tau in range(tau_max + 1):
+        
+        # Vectorized implementation: for each tau, compute all N*N correlations at once
+        def compute_corrs_for_tau(tau):
             effective_T = T - tau
-            for i in range(N):
-                for j in range(N):
-                    if tau == 0:
-                        X = data[:, i]
-                        Y = data[:, j]
-                    else:
-                        X = data[: T - tau, i]  # X at t - tau
-                        Y = data[tau:, j]  # Y at t
-
-                    corr_matrix = corr_matrix.at[i, j, tau].set(
-                        self._compute_correlation(X, Y)
-                    )
+            # For tau=0, use full data; for tau>0, use lagged data
+            X_data = jax.lax.cond(
+                tau == 0,
+                lambda: data,
+                lambda: data[: T - tau, :]  # X at t - tau
+            )[:effective_T, :]
+            Y_data = jax.lax.cond(
+                tau == 0,
+                lambda: data,
+                lambda: data[tau:, :]  # Y at t
+            )[:effective_T, :]
+            
+            # Center data for all variables at once
+            X_centered = X_data - jnp.mean(X_data, axis=0, keepdims=True)
+            Y_centered = Y_data - jnp.mean(Y_data, axis=0, keepdims=True)
+            
+            # Compute all pairwise correlations: corr[i,j] = sum(X_i * Y_j) / sqrt(sum(X_i^2) * sum(Y_j^2))
+            # Using matrix operations: (X^T @ Y) / outer(norm_X, norm_Y)
+            norms_X = jnp.sqrt(jnp.sum(X_centered**2, axis=0))
+            norms_Y = jnp.sqrt(jnp.sum(Y_centered**2, axis=0))
+            
+            # Handle zero norms
+            norms_X = jnp.where(norms_X > 1e-10, norms_X, 1.0)
+            norms_Y = jnp.where(norms_Y > 1e-10, norms_Y, 1.0)
+            
+            # Correlation matrix for this tau
+            cov_matrix = X_centered.T @ Y_centered / effective_T
+            corr_tau = cov_matrix / jnp.outer(norms_X, norms_Y) * effective_T
+            
+            return jnp.clip(corr_tau, -1.0, 1.0)
+        
+        # Stack results for all taus
+        corr_matrix = jnp.stack([compute_corrs_for_tau(tau) for tau in range(tau_max + 1)], axis=2)
 
         return corr_matrix
 
-    @partial(jax.jit, static_argnums=(0,))
     def compute_statistic_batch(
         self,
         X_batch: jax.Array,
@@ -369,13 +437,45 @@ class ParCorr(CondIndTest):
             Partial correlations, shape (n_tests,).
         """
         if Z_batch is None:
-            # Vectorized simple correlation
-            return jax.vmap(self._compute_correlation)(X_batch, Y_batch)
+            # Vectorized simple correlation using standalone function
+            return _batch_correlation_jit(X_batch, Y_batch)
+        else:
+            # Vectorized partial correlation using standalone function
+            return _batch_partial_correlation_jit(X_batch, Y_batch, Z_batch)
+
+    def run_batch(
+        self,
+        X_batch: jax.Array,
+        Y_batch: jax.Array,
+        Z_batch: Optional[jax.Array] = None,
+        alpha: Optional[float] = None,
+    ) -> Tuple[jax.Array, jax.Array]:
+        """
+        Optimized batch implementation for ParCorr.
+        
+        This overrides the base class implementation for maximum performance
+        by using module-level JIT'd functions instead of method-based JIT.
+        """
+        n_samples = jnp.asarray(X_batch.shape[1], dtype=jnp.int32)
+        n_conditions = jnp.asarray(
+            0 if Z_batch is None else (Z_batch.shape[2] if Z_batch.ndim == 3 else 1),
+            dtype=jnp.int32,
+        )
+        
+        # Compute statistics in a single vectorized call
+        if Z_batch is None:
+            statistics = _batch_correlation_jit(X_batch, Y_batch)
+        else:
+            statistics = _batch_partial_correlation_jit(X_batch, Y_batch, Z_batch)
+        
+        # Compute p-values
+        if self.significance == "analytic":
+            pvalues = _batch_pvalue_jit(statistics, n_samples, n_conditions)
         else:
-            # Vectorized partial correlation
-            return jax.vmap(self._compute_partial_correlation)(
-                X_batch, Y_batch, Z_batch
-            )
+            # Fall back to base class permutation method
+            pvalues = self._batch_permutation_pvalues(X_batch, Y_batch, Z_batch, statistics)
+        
+        return statistics, pvalues
 
     def __repr__(self) -> str:
         return (
diff --git a/jax_pcmci/parallel.py b/jax_pcmci/parallel.py
index e7caa51..59b7196 100644
--- a/jax_pcmci/parallel.py
+++ b/jax_pcmci/parallel.py
@@ -28,7 +28,7 @@ Example:
 
 from __future__ import annotations
 
-from typing import Callable, Optional, Sequence, Tuple, Union
+from typing import Callable, Dict, Optional, Sequence, Tuple, Union
 from dataclasses import dataclass
 from functools import partial
 
@@ -120,7 +120,7 @@ def get_optimal_chunk_size(
     compilation and intermediate computations.
     """
     # Bytes per element
-    bytes_per_elem = 8 if dtype == jnp.float64 else 4
+    bytes_per_elem = jnp.dtype(dtype).itemsize
     
     # Memory available (use 70% to leave headroom)
     available_bytes = memory_limit_gb * 1e9 * 0.7
@@ -174,48 +174,110 @@ def chunked_vmap(
     >>> results = batched_fn(large_batch)  # Processes in chunks of 1000
     """
     vmapped = jit(vmap(fn, in_axes=in_axes))
-    
-    def chunked_fn(*args):
-        # Determine batch size from first arg with batch axis
+
+    def _get_batch_size(args) -> int:
         if isinstance(in_axes, int):
-            batch_size = args[0].shape[in_axes]
-        else:
-            for i, ax in enumerate(in_axes):
-                if ax is not None:
-                    batch_size = args[i].shape[ax]
-                    break
-        
+            return args[0].shape[in_axes]
+        for i, ax in enumerate(in_axes):
+            if ax is not None:
+                return args[i].shape[ax]
+        raise ValueError("Could not determine batch axis from in_axes")
+
+    def _pad_arg(arg: jnp.ndarray, ax: Optional[int], pad_size: int) -> jnp.ndarray:
+        if ax is None or pad_size == 0:
+            return arg
+        pad_width = [(0, 0)] * arg.ndim
+        pad_width[ax] = (0, pad_size)
+        return jnp.pad(arg, pad_width)
+
+    def _slice_arg(arg: jnp.ndarray, ax: Optional[int], start: int) -> jnp.ndarray:
+        if ax is None:
+            return arg
+        start_indices = [0] * arg.ndim
+        start_indices[ax] = start
+        slice_sizes = list(arg.shape)
+        slice_sizes[ax] = chunk_size
+        return jax.lax.dynamic_slice(arg, start_indices, slice_sizes)
+
+    def chunked_fn(*args):
+        batch_size = _get_batch_size(args)
         if batch_size <= chunk_size:
             return vmapped(*args)
-        
-        # Split into chunks
+
         n_chunks = (batch_size + chunk_size - 1) // chunk_size
-        results = []
-        
-        for i in range(n_chunks):
-            start = i * chunk_size
-            end = min((i + 1) * chunk_size, batch_size)
-            
-            # Slice each batched argument
+        padded_size = n_chunks * chunk_size
+        pad_size = padded_size - batch_size
+
+        padded_args = []
+        for j, arg in enumerate(args):
+            ax = in_axes if isinstance(in_axes, int) else in_axes[j]
+            padded_args.append(_pad_arg(arg, ax, pad_size))
+
+        if progress_callback is not None:
+            # Python loop to allow progress callbacks
+            outputs = None
+            for i in range(n_chunks):
+                start = i * chunk_size
+                chunk_args = []
+                for j, arg in enumerate(padded_args):
+                    ax = in_axes if isinstance(in_axes, int) else in_axes[j]
+                    chunk_args.append(_slice_arg(arg, ax, start))
+
+                chunk_result = vmapped(*tuple(chunk_args))
+                if outputs is None:
+                    outputs = jax.tree_util.tree_map(
+                        lambda x: jnp.zeros((padded_size,) + x.shape[1:], x.dtype),
+                        chunk_result,
+                    )
+                outputs = jax.tree_util.tree_map(
+                    lambda out, res: jax.lax.dynamic_update_slice(
+                        out, res, (start,) + (0,) * (res.ndim - 1)
+                    ),
+                    outputs,
+                    chunk_result,
+                )
+                progress_callback(min(start + chunk_size, batch_size), batch_size)
+
+            return jax.tree_util.tree_map(lambda x: x[:batch_size], outputs)
+
+        # JAX-friendly scan for memory-efficient chunking
+        def scan_body(carry, idx):
+            start = idx * chunk_size
             chunk_args = []
-            for j, arg in enumerate(args):
+            for j, arg in enumerate(padded_args):
                 ax = in_axes if isinstance(in_axes, int) else in_axes[j]
-                if ax is not None:
-                    slices = [slice(None)] * arg.ndim
-                    slices[ax] = slice(start, end)
-                    chunk_args.append(arg[tuple(slices)])
-                else:
-                    chunk_args.append(arg)
-            
+                chunk_args.append(_slice_arg(arg, ax, start))
             chunk_result = vmapped(*tuple(chunk_args))
-            results.append(chunk_result)
-            
-            if progress_callback is not None:
-                progress_callback(end, batch_size)
-        
-        # Concatenate results
-        return jnp.concatenate(results, axis=0)
-    
+            updated = jax.tree_util.tree_map(
+                lambda out, res: jax.lax.dynamic_update_slice(
+                    out, res, (start,) + (0,) * (res.ndim - 1)
+                ),
+                carry,
+                chunk_result,
+            )
+            return updated, None
+
+        # Allocate output using the first chunk's shape
+        first_chunk_args = []
+        for j, arg in enumerate(padded_args):
+            ax = in_axes if isinstance(in_axes, int) else in_axes[j]
+            first_chunk_args.append(_slice_arg(arg, ax, 0))
+        first_result = vmapped(*tuple(first_chunk_args))
+        output_init = jax.tree_util.tree_map(
+            lambda x: jnp.zeros((padded_size,) + x.shape[1:], x.dtype),
+            first_result,
+        )
+        output_init = jax.tree_util.tree_map(
+            lambda out, res: jax.lax.dynamic_update_slice(
+                out, res, (0,) + (0,) * (res.ndim - 1)
+            ),
+            output_init,
+            first_result,
+        )
+
+        output_final, _ = jax.lax.scan(scan_body, output_init, jnp.arange(1, n_chunks))
+        return jax.tree_util.tree_map(lambda x: x[:batch_size], output_final)
+
     return chunked_fn
 
 
@@ -257,6 +319,15 @@ def parallel_map(
     """
     if config is None:
         config = ParallelConfig()
+
+    if config.n_devices is not None:
+        if config.n_devices < 1:
+            raise ValueError(f"n_devices must be >= 1, got {config.n_devices}")
+        available = jax.device_count()
+        if config.n_devices > available:
+            raise ValueError(
+                f"n_devices ({config.n_devices}) exceeds available devices ({available})"
+            )
     
     batch_size = data.shape[0]
     n_devices = jax.device_count()
@@ -517,7 +588,7 @@ def benchmark_parallel_modes(
     n_samples: int = 500,
     n_tests: int = 1000,
     seed: int = 42
-) -> dict[str, BenchmarkResult]:
+) -> Dict[str, BenchmarkResult]:
     """
     Benchmark different parallelization modes.
     
diff --git a/jax_pcmci/results.py b/jax_pcmci/results.py
index 2e2231e..76d3432 100644
--- a/jax_pcmci/results.py
+++ b/jax_pcmci/results.py
@@ -221,15 +221,21 @@ class PCMCIResults:
         sorted_idx = np.argsort(pvals_flat)
         sorted_pvals = pvals_flat[sorted_idx]
 
-        # Compute adjusted p-values
-        adjusted = np.zeros(n)
-        cummin = 1.0
-
-        for i in range(n - 1, -1, -1):
-            rank = i + 1
-            adjusted_p = sorted_pvals[i] * n / rank
-            cummin = min(cummin, adjusted_p)
-            adjusted[sorted_idx[i]] = min(cummin, 1.0)
+        # Compute adjusted p-values (vectorized)
+        # adjusted_p[i] = p[i] * n / (i+1)
+        ranks = np.arange(1, n + 1)
+        raw_adjusted = sorted_pvals * n / ranks
+        
+        # Apply cumulative minimum from the end (reverse cummin)
+        # np.minimum.accumulate on reversed array
+        cummin_adjusted = np.minimum.accumulate(raw_adjusted[::-1])[::-1]
+        
+        # Cap at 1.0
+        cummin_adjusted = np.minimum(cummin_adjusted, 1.0)
+        
+        # Put back in original order
+        adjusted = np.empty(n)
+        adjusted[sorted_idx] = cummin_adjusted
 
         return adjusted.reshape(shape)
 
diff --git a/pyproject.toml b/pyproject.toml
index c34f4f4..9eb2bc3 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -4,7 +4,7 @@ build-backend = "setuptools.build_meta"
 
 [project]
 name = "JAX-PCMCI"
-version = "1.0.0"
+version = "1.1.0"
 description = "High-performance causal discovery using PCMCI algorithms with JAX acceleration"
 readme = "README.md"
 license = {text = "MIT"}
diff --git a/tests/__pycache__/test_pcmci.cpython-313-pytest-9.0.2.pyc b/tests/__pycache__/test_pcmci.cpython-313-pytest-9.0.2.pyc
index 420015c..277ffc6 100644
Binary files a/tests/__pycache__/test_pcmci.cpython-313-pytest-9.0.2.pyc and b/tests/__pycache__/test_pcmci.cpython-313-pytest-9.0.2.pyc differ
diff --git a/wiki b/wiki
index 30c8347..0b9fbbc 160000
--- a/wiki
+++ b/wiki
@@ -1 +1 @@
-Subproject commit 30c83472c627c6cde2caecb18886b5ec03d28590
+Subproject commit 0b9fbbcea8a962e69dd61c01fb87b5580931b001
